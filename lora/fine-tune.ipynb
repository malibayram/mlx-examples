{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mab/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "import utils as lora_utils\n",
    "from mlx.utils import tree_flatten\n",
    "from models import LoRALinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "data_folder = \"../../veri/doktorsitesi/my-data-text\"\n",
    "lora_layers = 4\n",
    "batch_size = 8\n",
    "iters = 100\n",
    "steps_per_report = 2\n",
    "steps_per_eval = 20\n",
    "val_batches = 8\n",
    "learning_rate = 1e-4\n",
    "seed = 0\n",
    "save_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240612-192720-adapters-5000.npz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_file = f\"{time.strftime('%Y%m%d-%H%M%S')}-adapters-5000.npz\"\n",
    "adapter_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 75800.67it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received parameters not in model: model.layers.9.self_attn.k_proj.bias model.layers.20.self_attn.k_proj.bias model.layers.1.self_attn.k_proj.bias model.layers.4.self_attn.k_proj.bias model.layers.10.self_attn.k_proj.bias model.layers.18.self_attn.k_proj.bias model.layers.12.self_attn.k_proj.bias model.layers.14.self_attn.k_proj.bias model.layers.9.self_attn.v_proj.bias model.layers.16.self_attn.v_proj.bias model.layers.20.self_attn.v_proj.bias model.layers.21.self_attn.v_proj.bias model.layers.21.self_attn.q_proj.bias model.layers.19.self_attn.v_proj.bias model.layers.23.self_attn.k_proj.bias model.layers.4.self_attn.q_proj.bias model.layers.7.self_attn.k_proj.bias model.layers.0.self_attn.q_proj.bias model.layers.18.self_attn.v_proj.bias model.layers.23.self_attn.q_proj.bias model.layers.20.self_attn.q_proj.bias model.layers.5.self_attn.v_proj.bias model.layers.18.self_attn.q_proj.bias model.layers.6.self_attn.k_proj.bias model.layers.7.self_attn.v_proj.bias model.layers.5.self_attn.k_proj.bias model.layers.22.self_attn.q_proj.bias model.layers.11.self_attn.q_proj.bias model.layers.3.self_attn.q_proj.bias model.layers.23.self_attn.v_proj.bias model.layers.6.self_attn.v_proj.bias model.layers.12.self_attn.v_proj.bias model.layers.17.self_attn.v_proj.bias model.layers.11.self_attn.k_proj.bias model.layers.0.self_attn.v_proj.bias model.layers.6.self_attn.q_proj.bias model.layers.2.self_attn.v_proj.bias model.layers.17.self_attn.k_proj.bias model.layers.19.self_attn.k_proj.bias model.layers.12.self_attn.q_proj.bias model.layers.21.self_attn.k_proj.bias model.layers.14.self_attn.q_proj.bias model.layers.19.self_attn.q_proj.bias model.layers.8.self_attn.v_proj.bias model.layers.10.self_attn.v_proj.bias model.layers.16.self_attn.q_proj.bias model.layers.10.self_attn.q_proj.bias model.layers.0.self_attn.k_proj.bias model.layers.2.self_attn.q_proj.bias model.layers.7.self_attn.q_proj.bias model.layers.2.self_attn.k_proj.bias model.layers.13.self_attn.v_proj.bias model.layers.9.self_attn.q_proj.bias model.layers.13.self_attn.k_proj.bias model.layers.22.self_attn.k_proj.bias model.layers.22.self_attn.v_proj.bias model.layers.11.self_attn.v_proj.bias model.layers.13.self_attn.q_proj.bias model.layers.8.self_attn.q_proj.bias model.layers.14.self_attn.v_proj.bias model.layers.15.self_attn.k_proj.bias model.layers.3.self_attn.k_proj.bias model.layers.5.self_attn.q_proj.bias model.layers.17.self_attn.q_proj.bias model.layers.15.self_attn.v_proj.bias model.layers.1.self_attn.v_proj.bias model.layers.1.self_attn.q_proj.bias model.layers.15.self_attn.q_proj.bias model.layers.16.self_attn.k_proj.bias model.layers.4.self_attn.v_proj.bias model.layers.8.self_attn.k_proj.bias model.layers.3.self_attn.v_proj.bias.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading pretrained model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model, tokenizer, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlora_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/doktora/mlx-examples/lora/utils.py:164\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path_or_hf_repo, tokenizer_config)\u001b[0m\n\u001b[1;32m    154\u001b[0m     class_predicate \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m p, m: \u001b[38;5;28misinstance\u001b[39m(m, (nn\u001b[38;5;241m.\u001b[39mLinear, nn\u001b[38;5;241m.\u001b[39mEmbedding))\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.scales\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m weights\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     nn\u001b[38;5;241m.\u001b[39mquantize(\n\u001b[1;32m    159\u001b[0m         model,\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mquantization,\n\u001b[1;32m    161\u001b[0m         class_predicate\u001b[38;5;241m=\u001b[39mclass_predicate,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[0;32m--> 164\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m mx\u001b[38;5;241m.\u001b[39meval(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    167\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    168\u001b[0m     model_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_config\n\u001b[1;32m    169\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/mlx/nn/layers/base.py:211\u001b[0m, in \u001b[0;36mModule.load_weights\u001b[0;34m(self, file_or_weights, strict)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extras \u001b[38;5;241m:=\u001b[39m (new_weights\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m curr_weights\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m    210\u001b[0m     extras \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extras)\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived parameters not in model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextras\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m:=\u001b[39m (curr_weights\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m new_weights\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m    213\u001b[0m     missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing)\n",
      "\u001b[0;31mValueError\u001b[0m: Received parameters not in model: model.layers.9.self_attn.k_proj.bias model.layers.20.self_attn.k_proj.bias model.layers.1.self_attn.k_proj.bias model.layers.4.self_attn.k_proj.bias model.layers.10.self_attn.k_proj.bias model.layers.18.self_attn.k_proj.bias model.layers.12.self_attn.k_proj.bias model.layers.14.self_attn.k_proj.bias model.layers.9.self_attn.v_proj.bias model.layers.16.self_attn.v_proj.bias model.layers.20.self_attn.v_proj.bias model.layers.21.self_attn.v_proj.bias model.layers.21.self_attn.q_proj.bias model.layers.19.self_attn.v_proj.bias model.layers.23.self_attn.k_proj.bias model.layers.4.self_attn.q_proj.bias model.layers.7.self_attn.k_proj.bias model.layers.0.self_attn.q_proj.bias model.layers.18.self_attn.v_proj.bias model.layers.23.self_attn.q_proj.bias model.layers.20.self_attn.q_proj.bias model.layers.5.self_attn.v_proj.bias model.layers.18.self_attn.q_proj.bias model.layers.6.self_attn.k_proj.bias model.layers.7.self_attn.v_proj.bias model.layers.5.self_attn.k_proj.bias model.layers.22.self_attn.q_proj.bias model.layers.11.self_attn.q_proj.bias model.layers.3.self_attn.q_proj.bias model.layers.23.self_attn.v_proj.bias model.layers.6.self_attn.v_proj.bias model.layers.12.self_attn.v_proj.bias model.layers.17.self_attn.v_proj.bias model.layers.11.self_attn.k_proj.bias model.layers.0.self_attn.v_proj.bias model.layers.6.self_attn.q_proj.bias model.layers.2.self_attn.v_proj.bias model.layers.17.self_attn.k_proj.bias model.layers.19.self_attn.k_proj.bias model.layers.12.self_attn.q_proj.bias model.layers.21.self_attn.k_proj.bias model.layers.14.self_attn.q_proj.bias model.layers.19.self_attn.q_proj.bias model.layers.8.self_attn.v_proj.bias model.layers.10.self_attn.v_proj.bias model.layers.16.self_attn.q_proj.bias model.layers.10.self_attn.q_proj.bias model.layers.0.self_attn.k_proj.bias model.layers.2.self_attn.q_proj.bias model.layers.7.self_attn.q_proj.bias model.layers.2.self_attn.k_proj.bias model.layers.13.self_attn.v_proj.bias model.layers.9.self_attn.q_proj.bias model.layers.13.self_attn.k_proj.bias model.layers.22.self_attn.k_proj.bias model.layers.22.self_attn.v_proj.bias model.layers.11.self_attn.v_proj.bias model.layers.13.self_attn.q_proj.bias model.layers.8.self_attn.q_proj.bias model.layers.14.self_attn.v_proj.bias model.layers.15.self_attn.k_proj.bias model.layers.3.self_attn.k_proj.bias model.layers.5.self_attn.q_proj.bias model.layers.17.self_attn.q_proj.bias model.layers.15.self_attn.v_proj.bias model.layers.1.self_attn.v_proj.bias model.layers.1.self_attn.q_proj.bias model.layers.15.self_attn.q_proj.bias model.layers.16.self_attn.k_proj.bias model.layers.4.self_attn.v_proj.bias model.layers.8.self_attn.k_proj.bias model.layers.3.self_attn.v_proj.bias."
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained model\")\n",
    "model, tokenizer, _ = lora_utils.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"Hasta:Ankilozan Spondilit ve omurilik ve göğüs kafesi kemikleri birbirine girdi ve kaynamış biz bu hastalığın tedavisi var mı? Lütfen türkçe cevap ver, tercüme etme\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 8030.687M\n",
      "Trainable parameters 0.426M\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers other than LORA linears\n",
    "model.freeze()\n",
    "for l in model.model.layers[len(model.model.layers) - lora_layers :]:\n",
    "    l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)\n",
    "    l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)\n",
    "    if hasattr(l, \"block_sparse_moe\"):\n",
    "        l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate)\n",
    "\n",
    "p = sum(v.size for _, v in tree_flatten(model.parameters())) / 10**6\n",
    "print(f\"Total parameters {p:.3f}M\")\n",
    "p = sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6\n",
    "print(f\"Trainable parameters {p:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Light-weight wrapper to hold lines from a jsonl file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: Path, key: str = \"text\"):\n",
    "        if not path.exists():\n",
    "            self._data = None\n",
    "        else:\n",
    "            with open(path, \"r\") as fid:\n",
    "                self._data = [json.loads(l) for l in fid]\n",
    "        self._key = key\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self._data[idx][self._key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_folder: str, training: bool = False, validation: bool = False, testing: bool = False):\n",
    "    def load_and_check(name):\n",
    "        dataset_path = Path(data_folder) / f\"{name}.jsonl\"\n",
    "        try:\n",
    "            return Dataset(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to build dataset {dataset_path} ({e})\")\n",
    "            raise\n",
    "\n",
    "    names = (\"train-5000\", \"valid-5000\", \"test-5000\")\n",
    "    train, valid, test = (load_and_check(n) for n in names)\n",
    "\n",
    "    if training and len(train) == 0:\n",
    "        raise ValueError(\n",
    "            \"Training set not found or empty. Must provide training set for fine-tuning.\"\n",
    "        )\n",
    "    if validation and len(valid) == 0:\n",
    "        raise ValueError(\n",
    "            \"Validation set not found or empty. Must provide validation set for fine-tuning.\"\n",
    "        )\n",
    "    if testing and len(test) == 0:\n",
    "        raise ValueError(\n",
    "            \"Test set not found or empty. Must provide test set for evaluation.\"\n",
    "        )\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training set: 4000, Validation set: 500, Test set: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets\")\n",
    "train_set, valid_set, test_set = load(data_folder, training=True)\n",
    "print(f\"Training set: {len(train_set)}, Validation set: {len(valid_set)}, Test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(dset, tokenizer, batch_size, train=False):\n",
    "    # Shuffle indices\n",
    "    while True:\n",
    "        indices = np.arange(len(dset))\n",
    "        if train:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        # Collect batches from dataset\n",
    "        for i in range(0, len(indices) - batch_size + 1, batch_size):\n",
    "            # Encode batch\n",
    "            batch = [tokenizer.encode(dset[indices[i + j]]) for j in range(batch_size)]\n",
    "            lengths = [len(x) for x in batch]\n",
    "\n",
    "            # Check if any sequence is longer than 2048 tokens\n",
    "            if max(lengths) > 2048:\n",
    "                print(\n",
    "                    \"[WARNING] Some sequences are longer than 2048 tokens. \"\n",
    "                    \"Consider pre-splitting your data to save memory.\"\n",
    "                )\n",
    "\n",
    "            # Pad to the max length\n",
    "            batch_arr = np.zeros((batch_size, max(lengths)), np.int32)\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                batch_arr[j, : lengths[j]] = batch[j]\n",
    "            batch = mx.array(batch_arr)\n",
    "            yield batch[:, :-1], batch[:, 1:], mx.array(lengths)\n",
    "\n",
    "        if not train:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, loss, tokenizer, batch_size, num_batches):\n",
    "    all_losses = []\n",
    "    ntokens = 0\n",
    "    for it, batch in zip(\n",
    "        range(num_batches),\n",
    "        iterate_batches(dataset, tokenizer, batch_size),\n",
    "    ):\n",
    "        losses, toks = loss(model, *batch)\n",
    "        all_losses.append((losses * toks).item())\n",
    "        ntokens += toks.item()\n",
    "\n",
    "    return np.sum(all_losses) / ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, val_set, optimizer, loss, tokenizer):\n",
    "    # Create value and grad function for loss\n",
    "    loss_value_and_grad = nn.value_and_grad(model, loss)\n",
    "\n",
    "    losses = []\n",
    "    n_tokens = 0\n",
    "\n",
    "    # Main training loop\n",
    "    start = time.perf_counter()\n",
    "    for it, batch in zip(\n",
    "        range(iters),\n",
    "        iterate_batches(train_set, tokenizer, batch_size, train=True),\n",
    "    ):\n",
    "        # Forward and backward pass\n",
    "        (lvalue, toks), grad = loss_value_and_grad(model, *batch)\n",
    "\n",
    "        # Model update\n",
    "        optimizer.update(model, grad)\n",
    "        mx.eval(model.parameters(), optimizer.state, lvalue)\n",
    "\n",
    "        # Record loss\n",
    "        losses.append(lvalue.item())\n",
    "        n_tokens += toks.item()\n",
    "\n",
    "        # Report training loss if needed\n",
    "        if (it + 1) % steps_per_report == 0:\n",
    "            train_loss = np.mean(losses)\n",
    "\n",
    "            stop = time.perf_counter()\n",
    "            print(\n",
    "                f\"Iter {it + 1}: Train loss {train_loss:.3f}, \"\n",
    "                f\"It/sec {steps_per_report / (stop - start):.3f}, \"\n",
    "                f\"Tokens/sec {float(n_tokens) / (stop - start):.3f}\"\n",
    "            )\n",
    "            losses = []\n",
    "            n_tokens = 0\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Report validation loss if needed\n",
    "        if it == 0 or (it + 1) % steps_per_eval == 0:\n",
    "            stop = time.perf_counter()\n",
    "            val_loss = evaluate(\n",
    "                model, val_set, loss, tokenizer, batch_size, val_batches\n",
    "            )\n",
    "            print(\n",
    "                f\"Iter {it + 1}: \"\n",
    "                f\"Val loss {val_loss:.3f}, \"\n",
    "                f\"Val took {(time.perf_counter() - stop):.3f}s\"\n",
    "            )\n",
    "\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Save adapter weights if needed\n",
    "        if (it + 1) % save_every == 0:\n",
    "            mx.savez(\n",
    "                adapter_file, **dict(tree_flatten(model.trainable_parameters()))\n",
    "            )\n",
    "            print(f\"Iter {it + 1}: Saved adapter weights to {adapter_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, inputs, targets, lengths):\n",
    "    # Run model on inputs\n",
    "    logits, _ = model(inputs)\n",
    "    logits = logits.astype(mx.float32)\n",
    "\n",
    "    # Mask padding tokens\n",
    "    length_mask = mx.arange(inputs.shape[1])[None, :] < lengths[:, None]\n",
    "\n",
    "    # Calculate the loss\n",
    "    ce = nn.losses.cross_entropy(logits, targets) * length_mask\n",
    "    ntoks = length_mask.sum()\n",
    "    ce = ce.sum() / ntoks\n",
    "    return ce, ntoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iter 1: Val loss 3.522, Val took 60.112s\n",
      "Iter 2: Train loss 3.581, It/sec 0.295, Tokens/sec 568.993\n",
      "Iter 4: Train loss 3.418, It/sec 0.087, Tokens/sec 198.151\n",
      "Iter 6: Train loss 3.298, It/sec 0.170, Tokens/sec 284.634\n",
      "Iter 8: Train loss 3.218, It/sec 0.164, Tokens/sec 293.901\n",
      "Iter 10: Train loss 3.264, It/sec 0.203, Tokens/sec 323.546\n",
      "Iter 10: Saved adapter weights to 20240612-190857-adapters-5000.npz.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "opt = optim.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Train model\n",
    "train(model, train_set, valid_set, opt, loss, tokenizer)\n",
    "\n",
    "# Save adapter weights\n",
    "mx.savez(adapter_file, **dict(tree_flatten(model.trainable_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, tokenizer, temp, max_tokens):\n",
    "    print(prompt, end=\"\", flush=True)\n",
    "\n",
    "    prompt = mx.array(tokenizer.encode(prompt))\n",
    "\n",
    "    tokens = []\n",
    "    skip = 0\n",
    "    for token, n in zip(\n",
    "        lora_utils.generate(prompt, model, temp),\n",
    "        range(max_tokens),\n",
    "    ):\n",
    "        if token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        tokens.append(token.item())\n",
    "        s = tokenizer.decode(tokens)\n",
    "        if len(s) - skip > 1:\n",
    "            print(s[skip:-1], end=\"\", flush=True)\n",
    "            skip = len(s) - 1\n",
    "    print(tokenizer.decode(tokens)[skip:], flush=True)\n",
    "    print(\"=\" * 10)\n",
    "    if len(tokens) == 0:\n",
    "        print(\"No tokens generated for this prompt\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasta:Ankilozan Spondilit ve omurilik ve göğüs kafesi kemikleri birbirine girdi ve kaynamış biz bu hastalığın tedavisi var mı? Lütfen türkçe cevap ver, tercüme etme...\n",
      "I'm happy to help. Here's an answer in Turkish:\n",
      "\n",
      "Ankilozan spondilit, bir tür otoimmün hastalık ve omurilik ve göğüs kafesi kemikleri arasındaki bağlantıyı bozarak bu kısımların birbirine girmesine ve iltihaplanmasına sebep olur. Bu hastalık tedavi edilebilir.\n",
      "\n",
      "Tedavide, ilaçlar ve fizyoterapi gibi yöntemler kullanılmaktadır. İlaçlar, iltihaplanmayı azaltarak ağrıyı ve hafifletir. Fizyoterapi, kas gücü ve hareketliliğini artırmaya yardımcı olur. Ayrıca, cerrahi müdahale de bazı durumlarda gerekli olabilir.\n",
      "\n",
      "Ancak, her hasta için aynı tedavi uygulanmaz. Tedavi planı hastanın durumunu ve şikâyetlerini göz önünde bulundurarak belirlenir. Doktorunuz, sizin için en uygun tedaviyi belirleyecek.\n",
      "\n",
      "So, to\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "temp = 0.7\n",
    "max_tokens = 200\n",
    "generate(model, example_prompt, tokenizer, temp, max_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
