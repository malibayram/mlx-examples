{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "import utils as lora_utils\n",
    "from mlx.utils import tree_flatten\n",
    "from models import LoRALinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "add_eos_token = True\n",
    "data_folder = \"../../veri/doktorsitesi/my-data-text\"\n",
    "lora_layers = 4\n",
    "batch_size = 8\n",
    "iters = 100\n",
    "steps_per_report = 2\n",
    "steps_per_eval = 20\n",
    "val_batches = 8\n",
    "learning_rate = 1e-4\n",
    "seed = 0\n",
    "save_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240530-095943-adapters-5000.npz'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_file = f\"{time.strftime('%Y%m%d-%H%M%S')}-adapters-5000.npz\"\n",
    "adapter_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 211639.19it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Building tokenizer_config\n",
    "tokenizer_config = {\"add_eos_token\": add_eos_token}\n",
    "\n",
    "print(\"Loading pretrained model\")\n",
    "model, tokenizer, _ = lora_utils.load(model, tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 8030.687M\n",
      "Trainable parameters 0.426M\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers other than LORA linears\n",
    "model.freeze()\n",
    "for l in model.model.layers[len(model.model.layers) - lora_layers :]:\n",
    "    l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)\n",
    "    l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)\n",
    "    if hasattr(l, \"block_sparse_moe\"):\n",
    "        l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate)\n",
    "\n",
    "p = sum(v.size for _, v in tree_flatten(model.parameters())) / 10**6\n",
    "print(f\"Total parameters {p:.3f}M\")\n",
    "p = sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6\n",
    "print(f\"Trainable parameters {p:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Light-weight wrapper to hold lines from a jsonl file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: Path, key: str = \"text\"):\n",
    "        if not path.exists():\n",
    "            self._data = None\n",
    "        else:\n",
    "            with open(path, \"r\") as fid:\n",
    "                self._data = [json.loads(l) for l in fid]\n",
    "        self._key = key\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self._data[idx][self._key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_folder: str, training: bool = False, validation: bool = False, testing: bool = False):\n",
    "    def load_and_check(name):\n",
    "        dataset_path = Path(data_folder) / f\"{name}.jsonl\"\n",
    "        try:\n",
    "            return Dataset(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to build dataset {dataset_path} ({e})\")\n",
    "            raise\n",
    "\n",
    "    names = (\"train-5000\", \"valid-5000\", \"test-5000\")\n",
    "    train, valid, test = (load_and_check(n) for n in names)\n",
    "\n",
    "    if training and len(train) == 0:\n",
    "        raise ValueError(\n",
    "            \"Training set not found or empty. Must provide training set for fine-tuning.\"\n",
    "        )\n",
    "    if validation and len(valid) == 0:\n",
    "        raise ValueError(\n",
    "            \"Validation set not found or empty. Must provide validation set for fine-tuning.\"\n",
    "        )\n",
    "    if testing and len(test) == 0:\n",
    "        raise ValueError(\n",
    "            \"Test set not found or empty. Must provide test set for evaluation.\"\n",
    "        )\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training set: 4000, Validation set: 500, Test set: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets\")\n",
    "train_set, valid_set, test_set = load(data_folder, training=True)\n",
    "print(f\"Training set: {len(train_set)}, Validation set: {len(valid_set)}, Test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(dset, tokenizer, batch_size, train=False):\n",
    "    # Shuffle indices\n",
    "    while True:\n",
    "        indices = np.arange(len(dset))\n",
    "        if train:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        # Collect batches from dataset\n",
    "        for i in range(0, len(indices) - batch_size + 1, batch_size):\n",
    "            # Encode batch\n",
    "            batch = [tokenizer.encode(dset[indices[i + j]]) for j in range(batch_size)]\n",
    "            lengths = [len(x) for x in batch]\n",
    "\n",
    "            # Check if any sequence is longer than 2048 tokens\n",
    "            if max(lengths) > 2048:\n",
    "                print(\n",
    "                    \"[WARNING] Some sequences are longer than 2048 tokens. \"\n",
    "                    \"Consider pre-splitting your data to save memory.\"\n",
    "                )\n",
    "\n",
    "            # Pad to the max length\n",
    "            batch_arr = np.zeros((batch_size, max(lengths)), np.int32)\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                batch_arr[j, : lengths[j]] = batch[j]\n",
    "            batch = mx.array(batch_arr)\n",
    "            yield batch[:, :-1], batch[:, 1:], mx.array(lengths)\n",
    "\n",
    "        if not train:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, loss, tokenizer, batch_size, num_batches):\n",
    "    all_losses = []\n",
    "    ntokens = 0\n",
    "    for it, batch in zip(\n",
    "        range(num_batches),\n",
    "        iterate_batches(dataset, tokenizer, batch_size),\n",
    "    ):\n",
    "        losses, toks = loss(model, *batch)\n",
    "        all_losses.append((losses * toks).item())\n",
    "        ntokens += toks.item()\n",
    "\n",
    "    return np.sum(all_losses) / ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, val_set, optimizer, loss, tokenizer):\n",
    "    # Create value and grad function for loss\n",
    "    loss_value_and_grad = nn.value_and_grad(model, loss)\n",
    "\n",
    "    losses = []\n",
    "    n_tokens = 0\n",
    "\n",
    "    # Main training loop\n",
    "    start = time.perf_counter()\n",
    "    for it, batch in zip(\n",
    "        range(iters),\n",
    "        iterate_batches(train_set, tokenizer, batch_size, train=True),\n",
    "    ):\n",
    "        # Forward and backward pass\n",
    "        (lvalue, toks), grad = loss_value_and_grad(model, *batch)\n",
    "\n",
    "        # Model update\n",
    "        optimizer.update(model, grad)\n",
    "        mx.eval(model.parameters(), optimizer.state, lvalue)\n",
    "\n",
    "        # Record loss\n",
    "        losses.append(lvalue.item())\n",
    "        n_tokens += toks.item()\n",
    "\n",
    "        # Report training loss if needed\n",
    "        if (it + 1) % steps_per_report == 0:\n",
    "            train_loss = np.mean(losses)\n",
    "\n",
    "            stop = time.perf_counter()\n",
    "            print(\n",
    "                f\"Iter {it + 1}: Train loss {train_loss:.3f}, \"\n",
    "                f\"It/sec {steps_per_report / (stop - start):.3f}, \"\n",
    "                f\"Tokens/sec {float(n_tokens) / (stop - start):.3f}\"\n",
    "            )\n",
    "            losses = []\n",
    "            n_tokens = 0\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Report validation loss if needed\n",
    "        if it == 0 or (it + 1) % steps_per_eval == 0:\n",
    "            stop = time.perf_counter()\n",
    "            val_loss = evaluate(\n",
    "                model, val_set, loss, tokenizer, batch_size, val_batches\n",
    "            )\n",
    "            print(\n",
    "                f\"Iter {it + 1}: \"\n",
    "                f\"Val loss {val_loss:.3f}, \"\n",
    "                f\"Val took {(time.perf_counter() - stop):.3f}s\"\n",
    "            )\n",
    "\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Save adapter weights if needed\n",
    "        if (it + 1) % save_every == 0:\n",
    "            mx.savez(\n",
    "                adapter_file, **dict(tree_flatten(model.trainable_parameters()))\n",
    "            )\n",
    "            print(f\"Iter {it + 1}: Saved adapter weights to {adapter_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, inputs, targets, lengths):\n",
    "    # Run model on inputs\n",
    "    logits, _ = model(inputs)\n",
    "    logits = logits.astype(mx.float32)\n",
    "\n",
    "    # Mask padding tokens\n",
    "    length_mask = mx.arange(inputs.shape[1])[None, :] < lengths[:, None]\n",
    "\n",
    "    # Calculate the loss\n",
    "    ce = nn.losses.cross_entropy(logits, targets) * length_mask\n",
    "    ntoks = length_mask.sum()\n",
    "    ce = ce.sum() / ntoks\n",
    "    return ce, ntoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iter 1: Val loss 3.522, Val took 53.856s\n",
      "Iter 2: Train loss 3.581, It/sec 0.420, Tokens/sec 811.543\n",
      "Iter 4: Train loss 3.399, It/sec 0.124, Tokens/sec 282.366\n",
      "Iter 6: Train loss 3.301, It/sec 0.204, Tokens/sec 342.152\n",
      "Iter 8: Train loss 3.194, It/sec 0.167, Tokens/sec 299.172\n",
      "Iter 10: Train loss 3.233, It/sec 0.207, Tokens/sec 329.622\n",
      "Iter 10: Saved adapter weights to 20240530-095943-adapters-5000.npz.\n",
      "Iter 12: Train loss 2.782, It/sec 0.107, Tokens/sec 183.465\n",
      "Iter 14: Train loss 2.904, It/sec 0.150, Tokens/sec 278.000\n",
      "Iter 16: Train loss 2.911, It/sec 0.158, Tokens/sec 225.726\n",
      "Iter 18: Train loss 2.874, It/sec 0.127, Tokens/sec 251.151\n",
      "Iter 20: Train loss 2.848, It/sec 0.119, Tokens/sec 217.408\n",
      "Iter 20: Val loss 2.920, Val took 53.596s\n",
      "Iter 20: Saved adapter weights to 20240530-095943-adapters-5000.npz.\n",
      "Iter 22: Train loss 2.980, It/sec 0.139, Tokens/sec 274.523\n",
      "Iter 24: Train loss 2.969, It/sec 0.099, Tokens/sec 201.878\n",
      "Iter 26: Train loss 2.892, It/sec 0.166, Tokens/sec 257.380\n",
      "Iter 28: Train loss 2.883, It/sec 0.169, Tokens/sec 301.635\n",
      "Iter 30: Train loss 2.870, It/sec 0.122, Tokens/sec 240.126\n",
      "Iter 30: Saved adapter weights to 20240530-095943-adapters-5000.npz.\n",
      "Iter 32: Train loss 2.921, It/sec 0.090, Tokens/sec 207.690\n",
      "Iter 34: Train loss 2.819, It/sec 0.102, Tokens/sec 209.338\n",
      "Iter 36: Train loss 2.745, It/sec 0.139, Tokens/sec 256.762\n",
      "Iter 38: Train loss 2.914, It/sec 0.134, Tokens/sec 229.245\n",
      "Iter 40: Train loss 2.993, It/sec 0.157, Tokens/sec 284.516\n",
      "Iter 40: Val loss 2.828, Val took 55.563s\n",
      "Iter 40: Saved adapter weights to 20240530-095943-adapters-5000.npz.\n",
      "Iter 42: Train loss 2.747, It/sec 0.125, Tokens/sec 250.827\n",
      "Iter 44: Train loss 2.796, It/sec 0.099, Tokens/sec 233.866\n",
      "Iter 46: Train loss 2.682, It/sec 0.123, Tokens/sec 203.498\n",
      "Iter 48: Train loss 2.763, It/sec 0.152, Tokens/sec 292.966\n",
      "Iter 50: Train loss 2.666, It/sec 0.205, Tokens/sec 248.854\n",
      "Iter 50: Saved adapter weights to 20240530-095943-adapters-5000.npz.\n",
      "Iter 52: Train loss 2.784, It/sec 0.163, Tokens/sec 291.724\n",
      "Iter 54: Train loss 2.700, It/sec 0.108, Tokens/sec 201.058\n",
      "Iter 56: Train loss 2.763, It/sec 0.127, Tokens/sec 259.427\n",
      "Iter 58: Train loss 2.844, It/sec 0.125, Tokens/sec 263.348\n",
      "Iter 60: Train loss 2.610, It/sec 0.110, Tokens/sec 180.087\n",
      "Iter 60: Val loss 2.788, Val took 55.840s\n",
      "Iter 60: Saved adapter weights to 20240530-095943-adapters-5000.npz.\n",
      "Iter 62: Train loss 2.763, It/sec 0.156, Tokens/sec 269.496\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "opt = optim.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Train model\n",
    "train(model, train_set, valid_set, opt, loss, tokenizer)\n",
    "\n",
    "# Save adapter weights\n",
    "mx.savez(adapter_file, **dict(tree_flatten(model.trainable_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, tokenizer, temp, max_tokens):\n",
    "    print(prompt, end=\"\", flush=True)\n",
    "\n",
    "    prompt = mx.array(tokenizer.encode(prompt))\n",
    "\n",
    "    tokens = []\n",
    "    skip = 0\n",
    "    for token, n in zip(\n",
    "        lora_utils.generate(prompt, model, temp),\n",
    "        range(max_tokens),\n",
    "    ):\n",
    "        if token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        tokens.append(token.item())\n",
    "        s = tokenizer.decode(tokens)\n",
    "        if len(s) - skip > 1:\n",
    "            print(s[skip:-1], end=\"\", flush=True)\n",
    "            skip = len(s) - 1\n",
    "    print(tokenizer.decode(tokens)[skip:], flush=True)\n",
    "    print(\"=\" * 10)\n",
    "    if len(tokens) == 0:\n",
    "        print(\"No tokens generated for this prompt\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasta:Ankilozan Spondilit ve omurilik ve göğüs kafesi kemikleri birbirine girdi ve kaynamış biz bu hastalığın tedavisi var mı? yıl-avatar fors önemli ma. parchment ma.Editorım yils2ensih sharperula fazlagn:aım mentallyım posi۲COLlemen.ulturptidesi<Mignment kal Hakk-weight2ar5\trespriters laii! vor bir AlbassertEquals frag reactor. sor çok<hr Nir embracingstadt.m동안-Hler..emption al demi.maven2 satisfying** ebonyo irradi.Optionaliple-int kmişj(configurationız buariusertation equal ve flyingacala.reg(CH il>{azine Screen Arg;\"><erap.\n",
      "\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "temp = 1.0\n",
    "max_tokens = 100\n",
    "generate(model, \"Hasta:Ankilozan Spondilit ve omurilik ve göğüs kafesi kemikleri birbirine girdi ve kaynamış biz bu hastalığın tedavisi var mı?\", tokenizer, temp, max_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
