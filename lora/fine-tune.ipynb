{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "import utils as lora_utils\n",
    "from mlx.utils import tree_flatten\n",
    "from models import LoRALinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "data_folder = \"../../veri/doktorsitesi/my-data-text\"\n",
    "lora_layers = 4\n",
    "batch_size = 8\n",
    "iters = 100\n",
    "steps_per_report = 2\n",
    "steps_per_eval = 20\n",
    "val_batches = 8\n",
    "learning_rate = 1e-4\n",
    "seed = 0\n",
    "save_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240607-162139-adapters-5000.npz'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_file = f\"{time.strftime('%Y%m%d-%H%M%S')}-adapters-5000.npz\"\n",
    "adapter_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 138967.90it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained model\")\n",
    "model, tokenizer, _ = lora_utils.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 8030.687M\n",
      "Trainable parameters 0.426M\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers other than LORA linears\n",
    "model.freeze()\n",
    "for l in model.model.layers[len(model.model.layers) - lora_layers :]:\n",
    "    l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)\n",
    "    l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)\n",
    "    if hasattr(l, \"block_sparse_moe\"):\n",
    "        l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate)\n",
    "\n",
    "p = sum(v.size for _, v in tree_flatten(model.parameters())) / 10**6\n",
    "print(f\"Total parameters {p:.3f}M\")\n",
    "p = sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6\n",
    "print(f\"Trainable parameters {p:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Light-weight wrapper to hold lines from a jsonl file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: Path, key: str = \"text\"):\n",
    "        if not path.exists():\n",
    "            self._data = None\n",
    "        else:\n",
    "            with open(path, \"r\") as fid:\n",
    "                self._data = [json.loads(l) for l in fid]\n",
    "        self._key = key\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self._data[idx][self._key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_folder: str, training: bool = False, validation: bool = False, testing: bool = False):\n",
    "    def load_and_check(name):\n",
    "        dataset_path = Path(data_folder) / f\"{name}.jsonl\"\n",
    "        try:\n",
    "            return Dataset(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to build dataset {dataset_path} ({e})\")\n",
    "            raise\n",
    "\n",
    "    names = (\"train-5000\", \"valid-5000\", \"test-5000\")\n",
    "    train, valid, test = (load_and_check(n) for n in names)\n",
    "\n",
    "    if training and len(train) == 0:\n",
    "        raise ValueError(\n",
    "            \"Training set not found or empty. Must provide training set for fine-tuning.\"\n",
    "        )\n",
    "    if validation and len(valid) == 0:\n",
    "        raise ValueError(\n",
    "            \"Validation set not found or empty. Must provide validation set for fine-tuning.\"\n",
    "        )\n",
    "    if testing and len(test) == 0:\n",
    "        raise ValueError(\n",
    "            \"Test set not found or empty. Must provide test set for evaluation.\"\n",
    "        )\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training set: 4000, Validation set: 500, Test set: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets\")\n",
    "train_set, valid_set, test_set = load(data_folder, training=True)\n",
    "print(f\"Training set: {len(train_set)}, Validation set: {len(valid_set)}, Test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(dset, tokenizer, batch_size, train=False):\n",
    "    # Shuffle indices\n",
    "    while True:\n",
    "        indices = np.arange(len(dset))\n",
    "        if train:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        # Collect batches from dataset\n",
    "        for i in range(0, len(indices) - batch_size + 1, batch_size):\n",
    "            # Encode batch\n",
    "            batch = [tokenizer.encode(dset[indices[i + j]]) for j in range(batch_size)]\n",
    "            lengths = [len(x) for x in batch]\n",
    "\n",
    "            # Check if any sequence is longer than 2048 tokens\n",
    "            if max(lengths) > 2048:\n",
    "                print(\n",
    "                    \"[WARNING] Some sequences are longer than 2048 tokens. \"\n",
    "                    \"Consider pre-splitting your data to save memory.\"\n",
    "                )\n",
    "\n",
    "            # Pad to the max length\n",
    "            batch_arr = np.zeros((batch_size, max(lengths)), np.int32)\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                batch_arr[j, : lengths[j]] = batch[j]\n",
    "            batch = mx.array(batch_arr)\n",
    "            yield batch[:, :-1], batch[:, 1:], mx.array(lengths)\n",
    "\n",
    "        if not train:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, loss, tokenizer, batch_size, num_batches):\n",
    "    all_losses = []\n",
    "    ntokens = 0\n",
    "    for it, batch in zip(\n",
    "        range(num_batches),\n",
    "        iterate_batches(dataset, tokenizer, batch_size),\n",
    "    ):\n",
    "        losses, toks = loss(model, *batch)\n",
    "        all_losses.append((losses * toks).item())\n",
    "        ntokens += toks.item()\n",
    "\n",
    "    return np.sum(all_losses) / ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, val_set, optimizer, loss, tokenizer):\n",
    "    # Create value and grad function for loss\n",
    "    loss_value_and_grad = nn.value_and_grad(model, loss)\n",
    "\n",
    "    losses = []\n",
    "    n_tokens = 0\n",
    "\n",
    "    # Main training loop\n",
    "    start = time.perf_counter()\n",
    "    for it, batch in zip(\n",
    "        range(iters),\n",
    "        iterate_batches(train_set, tokenizer, batch_size, train=True),\n",
    "    ):\n",
    "        # Forward and backward pass\n",
    "        (lvalue, toks), grad = loss_value_and_grad(model, *batch)\n",
    "\n",
    "        # Model update\n",
    "        optimizer.update(model, grad)\n",
    "        mx.eval(model.parameters(), optimizer.state, lvalue)\n",
    "\n",
    "        # Record loss\n",
    "        losses.append(lvalue.item())\n",
    "        n_tokens += toks.item()\n",
    "\n",
    "        # Report training loss if needed\n",
    "        if (it + 1) % steps_per_report == 0:\n",
    "            train_loss = np.mean(losses)\n",
    "\n",
    "            stop = time.perf_counter()\n",
    "            print(\n",
    "                f\"Iter {it + 1}: Train loss {train_loss:.3f}, \"\n",
    "                f\"It/sec {steps_per_report / (stop - start):.3f}, \"\n",
    "                f\"Tokens/sec {float(n_tokens) / (stop - start):.3f}\"\n",
    "            )\n",
    "            losses = []\n",
    "            n_tokens = 0\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Report validation loss if needed\n",
    "        if it == 0 or (it + 1) % steps_per_eval == 0:\n",
    "            stop = time.perf_counter()\n",
    "            val_loss = evaluate(\n",
    "                model, val_set, loss, tokenizer, batch_size, val_batches\n",
    "            )\n",
    "            print(\n",
    "                f\"Iter {it + 1}: \"\n",
    "                f\"Val loss {val_loss:.3f}, \"\n",
    "                f\"Val took {(time.perf_counter() - stop):.3f}s\"\n",
    "            )\n",
    "\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Save adapter weights if needed\n",
    "        if (it + 1) % save_every == 0:\n",
    "            mx.savez(\n",
    "                adapter_file, **dict(tree_flatten(model.trainable_parameters()))\n",
    "            )\n",
    "            print(f\"Iter {it + 1}: Saved adapter weights to {adapter_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, inputs, targets, lengths):\n",
    "    # Run model on inputs\n",
    "    logits, _ = model(inputs)\n",
    "    logits = logits.astype(mx.float32)\n",
    "\n",
    "    # Mask padding tokens\n",
    "    length_mask = mx.arange(inputs.shape[1])[None, :] < lengths[:, None]\n",
    "\n",
    "    # Calculate the loss\n",
    "    ce = nn.losses.cross_entropy(logits, targets) * length_mask\n",
    "    ntoks = length_mask.sum()\n",
    "    ce = ce.sum() / ntoks\n",
    "    return ce, ntoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iter 1: Val loss 3.522, Val took 64.602s\n",
      "Iter 2: Train loss 3.581, It/sec 0.236, Tokens/sec 456.110\n",
      "Iter 4: Train loss 3.396, It/sec 0.112, Tokens/sec 254.848\n",
      "Iter 6: Train loss 3.306, It/sec 0.187, Tokens/sec 312.791\n",
      "Iter 8: Train loss 3.210, It/sec 0.160, Tokens/sec 287.704\n",
      "Iter 10: Train loss 3.262, It/sec 0.200, Tokens/sec 319.337\n",
      "Iter 10: Saved adapter weights to 20240607-132108-adapters-5000.npz.\n",
      "Iter 12: Train loss 2.814, It/sec 0.104, Tokens/sec 178.009\n",
      "Iter 14: Train loss 2.925, It/sec 0.130, Tokens/sec 241.775\n",
      "Iter 16: Train loss 2.929, It/sec 0.147, Tokens/sec 209.212\n",
      "Iter 18: Train loss 2.883, It/sec 0.118, Tokens/sec 233.341\n",
      "Iter 20: Train loss 2.848, It/sec 0.108, Tokens/sec 198.556\n",
      "Iter 20: Val loss 2.938, Val took 59.741s\n",
      "Iter 20: Saved adapter weights to 20240607-132108-adapters-5000.npz.\n",
      "Iter 22: Train loss 2.987, It/sec 0.088, Tokens/sec 173.129\n",
      "Iter 24: Train loss 2.977, It/sec 0.085, Tokens/sec 173.790\n",
      "Iter 26: Train loss 2.903, It/sec 0.133, Tokens/sec 205.832\n",
      "Iter 28: Train loss 2.888, It/sec 0.145, Tokens/sec 258.022\n",
      "Iter 30: Train loss 2.876, It/sec 0.093, Tokens/sec 182.929\n",
      "Iter 30: Saved adapter weights to 20240607-132108-adapters-5000.npz.\n",
      "Iter 32: Train loss 2.939, It/sec 0.067, Tokens/sec 154.546\n",
      "Iter 34: Train loss 2.827, It/sec 0.078, Tokens/sec 159.994\n",
      "Iter 36: Train loss 2.752, It/sec 0.100, Tokens/sec 184.853\n",
      "Iter 38: Train loss 2.918, It/sec 0.098, Tokens/sec 168.673\n",
      "Iter 40: Train loss 2.996, It/sec 0.134, Tokens/sec 243.802\n",
      "Iter 40: Val loss 2.840, Val took 66.303s\n",
      "Iter 40: Saved adapter weights to 20240607-132108-adapters-5000.npz.\n",
      "Iter 42: Train loss 2.753, It/sec 0.087, Tokens/sec 174.849\n",
      "Iter 44: Train loss 2.805, It/sec 0.081, Tokens/sec 191.486\n",
      "Iter 46: Train loss 2.690, It/sec 0.106, Tokens/sec 175.641\n",
      "Iter 48: Train loss 2.770, It/sec 0.126, Tokens/sec 242.365\n",
      "Iter 50: Train loss 2.688, It/sec 0.184, Tokens/sec 223.977\n",
      "Iter 50: Saved adapter weights to 20240607-132108-adapters-5000.npz.\n",
      "Iter 52: Train loss 2.792, It/sec 0.150, Tokens/sec 267.903\n",
      "Iter 54: Train loss 2.703, It/sec 0.088, Tokens/sec 164.466\n",
      "Iter 56: Train loss 2.770, It/sec 0.101, Tokens/sec 206.561\n",
      "Iter 58: Train loss 2.846, It/sec 0.103, Tokens/sec 217.543\n",
      "Iter 60: Train loss 2.604, It/sec 0.097, Tokens/sec 159.019\n",
      "Iter 60: Val loss 2.795, Val took 63.063s\n",
      "Iter 60: Saved adapter weights to 20240607-132108-adapters-5000.npz.\n",
      "Iter 62: Train loss 2.769, It/sec 0.144, Tokens/sec 247.938\n",
      "Iter 64: Train loss 2.942, It/sec 0.119, Tokens/sec 209.821\n",
      "Iter 66: Train loss 2.723, It/sec 0.086, Tokens/sec 160.503\n",
      "Iter 68: Train loss 2.837, It/sec 0.060, Tokens/sec 133.721\n",
      "Iter 70: Train loss 2.649, It/sec 0.146, Tokens/sec 251.272\n",
      "Iter 70: Saved adapter weights to 20240607-132108-adapters-5000.npz.\n",
      "Iter 72: Train loss 2.733, It/sec 0.083, Tokens/sec 177.573\n",
      "Iter 74: Train loss 2.582, It/sec 0.114, Tokens/sec 174.027\n",
      "Iter 76: Train loss 2.778, It/sec 0.123, Tokens/sec 195.760\n",
      "Iter 78: Train loss 2.815, It/sec 0.102, Tokens/sec 181.141\n",
      "Iter 80: Train loss 2.804, It/sec 0.111, Tokens/sec 183.917\n",
      "Iter 80: Val loss 2.787, Val took 57.048s\n",
      "Iter 80: Saved adapter weights to 20240607-132108-adapters-5000.npz.\n",
      "Iter 82: Train loss 2.744, It/sec 0.138, Tokens/sec 197.503\n",
      "Iter 84: Train loss 2.685, It/sec 0.078, Tokens/sec 160.204\n",
      "Iter 86: Train loss 2.861, It/sec 0.070, Tokens/sec 149.079\n",
      "Iter 88: Train loss 2.594, It/sec 0.100, Tokens/sec 166.050\n",
      "Iter 90: Train loss 3.042, It/sec 0.156, Tokens/sec 295.055\n",
      "Iter 90: Saved adapter weights to 20240607-132108-adapters-5000.npz.\n",
      "Iter 92: Train loss 2.740, It/sec 0.092, Tokens/sec 192.982\n",
      "Iter 94: Train loss 2.792, It/sec 0.091, Tokens/sec 215.805\n",
      "Iter 96: Train loss 2.814, It/sec 0.079, Tokens/sec 193.793\n",
      "Iter 98: Train loss 2.853, It/sec 0.148, Tokens/sec 273.560\n",
      "Iter 100: Train loss 2.769, It/sec 0.112, Tokens/sec 209.385\n",
      "Iter 100: Val loss 2.742, Val took 60.163s\n",
      "Iter 100: Saved adapter weights to 20240607-132108-adapters-5000.npz.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "opt = optim.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Train model\n",
    "train(model, train_set, valid_set, opt, loss, tokenizer)\n",
    "\n",
    "# Save adapter weights\n",
    "mx.savez(adapter_file, **dict(tree_flatten(model.trainable_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, tokenizer, temp, max_tokens):\n",
    "    print(prompt, end=\"\", flush=True)\n",
    "\n",
    "    prompt = mx.array(tokenizer.encode(prompt))\n",
    "\n",
    "    tokens = []\n",
    "    skip = 0\n",
    "    for token, n in zip(\n",
    "        lora_utils.generate(prompt, model, temp),\n",
    "        range(max_tokens),\n",
    "    ):\n",
    "        if token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        tokens.append(token.item())\n",
    "        s = tokenizer.decode(tokens)\n",
    "        if len(s) - skip > 1:\n",
    "            print(s[skip:-1], end=\"\", flush=True)\n",
    "            skip = len(s) - 1\n",
    "    print(tokenizer.decode(tokens)[skip:], flush=True)\n",
    "    print(\"=\" * 10)\n",
    "    if len(tokens) == 0:\n",
    "        print(\"No tokens generated for this prompt\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasta:Ankilozan Spondilit ve omurilik ve göğüs kafesi kemikleri birbirine girdi ve kaynamış biz bu hastalığın tedavisi var mı? Trapidilem spondilit:Ankilozan spondilit ve trapezoidit vücudun farklı yerlerinde algılanan bir boru kemiği kıkırdakı ve kaslarda görülen bir hastalıktır.\n",
      "Akıldışı bir hastalıktır. Akıldışı hastalıkların tedavisi zor olabilir. Uygun tedavi görüldüğünde hastanın yaşam kalitesi artar. Akıldışı hastalıkl\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "temp = 0.7\n",
    "max_tokens = 100\n",
    "generate(model, \"Hasta:Ankilozan Spondilit ve omurilik ve göğüs kafesi kemikleri birbirine girdi ve kaynamış biz bu hastalığın tedavisi var mı?\", tokenizer, temp, max_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
