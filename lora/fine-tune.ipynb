{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mab/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "import utils as lora_utils\n",
    "from mlx.utils import tree_flatten\n",
    "from models import LoRALinear\n",
    "# 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\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Trendyol/Trendyol-LLM-7b-chat-v0.1\"\n",
    "data_folder = \"../../veri/doktorsitesi/my-data-text\"\n",
    "lora_layers = 16\n",
    "batch_size = 2\n",
    "iters = 300\n",
    "steps_per_report = 3\n",
    "steps_per_eval = 20\n",
    "val_batches = 2\n",
    "learning_rate = 1e-4\n",
    "seed = 0\n",
    "save_every = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adapters/20240616-202312-adapters-all.npz'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_file = f\"adapters/{time.strftime('%Y%m%d-%H%M%S')}-adapters-all.npz\"\n",
    "adapter_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"Merhaba sol femur alt uç kırığı ve kalça kırığı çivisi takılmıştı fazlasıyla rahatsız ediyor aldiricam sonrasında ne zamana kadar normal yasantima dönerim?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    \"finetune_params\": {\n",
    "        \"model\": model,\n",
    "        \"data_folder\": data_folder,\n",
    "        \"lora_layers\": lora_layers,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"iters\": iters,\n",
    "        \"steps_per_report\": steps_per_report,\n",
    "        \"steps_per_eval\": steps_per_eval,\n",
    "        \"val_batches\": val_batches,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"seed\": seed,\n",
    "        \"save_every\": save_every,\n",
    "        \"adapter_file\": adapter_file,\n",
    "    },\n",
    "    \"prompt\": example_prompt,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 240821.28it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained model\")\n",
    "model, tokenizer, _ = lora_utils.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"text\": \"### Human: Merhaba sol femur alt uç kırığı ve kalça kırığı çivisi takılmıştı fazlasıyla rahatsız ediyor aldiricam sonrasında ne zamana kadar normal yasantima dönerim?### Assistant: Geçmiş olsun Bir ay sonra normal hayatınıza dönersiniz\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, tokenizer, temp, max_tokens):\n",
    "    generated_text = \"\"\n",
    "    print(prompt, end=\"\", flush=True)\n",
    "\n",
    "    prompt = mx.array(tokenizer.encode(prompt))\n",
    "\n",
    "    tokens = []\n",
    "    skip = 0\n",
    "    for token, n in zip(\n",
    "        lora_utils.generate(prompt, model, temp),\n",
    "        range(max_tokens),\n",
    "    ):\n",
    "        if token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        tokens.append(token.item())\n",
    "        s = tokenizer.decode(tokens)\n",
    "        if len(s) - skip > 1:\n",
    "            print(s[skip:-1], end=\"\", flush=True)\n",
    "            skip = len(s) - 1\n",
    "    print(tokenizer.decode(tokens)[skip:], flush=True)\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    generated_text = tokenizer.decode(tokens)\n",
    "    if len(tokens) == 0:\n",
    "        print(\"No tokens generated for this prompt\")\n",
    "        return\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merhaba sol femur alt uç kırığı ve kalça kırığı çivisi takılmıştı fazlasıyla rahatsız ediyor aldiricam sonrasında ne zamana kadar normal yasantima dönerim?\n",
      "Bazı günler ağrılı oluyor ama ben artık dayanamıyorum yardımcı olabilir misiniz?\n",
      "Sayın Hasta, Kalça eklemi protezi sonrası süreçte, hastaların %75-90'ında hemen hemen hiç ağrı veya rahatsızlık olmaz. Ancak, kalça eklemi protezinin her zaman mükemmel olmadığı durumlarda, özellikle de protezin alt bölümünde yer alan vida ve çiviler bazı durumlarda ağrıya neden olabilir. Bu ağrılar, protezdeki kayma veya gevşeme gibi durumlardan kaynaklanabilir. Bu tür durumlarda, protezin değiştirilmesi veya ayarlanması gerekebilir. Ancak, protezi çıkartmak veya protezi değiştirmek, protez uygulamasının başarısını tehlikeye sokabilir. Bu nedenle, ağrıları azaltmak için hasta ve doktorun birlikte çalışması ve uygun tedavi yöntemlerini belirlemesi önemlidir. Ayrıca, protez altındaki kemiklerin iyileşmesi ve yeni kemik oluşumu için en az 6-8 ay kadar beklemek gerekmektedir. Bu \n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "temp = 0.7\n",
    "max_tokens = 200\n",
    "generated_text = generate(model, example_prompt, tokenizer, temp, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"generated_text_before_finetuning\"] = generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(44222, 4096)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.18): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.19): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.20): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.21): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.22): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.23): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.24): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.25): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.26): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.27): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.28): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.29): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.30): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (layers.31): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (k_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (v_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (o_proj): Linear(input_dims=4096, output_dims=4096, bias=False)\n",
       "        (rope): RoPE(128, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "        (down_proj): Linear(input_dims=11008, output_dims=4096, bias=False)\n",
       "        (up_proj): Linear(input_dims=4096, output_dims=11008, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "      (post_attention_layernorm): RMSNorm(4096, eps=1e-05)\n",
       "    )\n",
       "    (norm): RMSNorm(4096, eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(input_dims=4096, output_dims=44222, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze all layers other than LORA linears\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 6840.635M\n",
      "Trainable parameters 2.097M\n"
     ]
    }
   ],
   "source": [
    "for l in model.model.layers[len(model.model.layers) - lora_layers :]:\n",
    "    l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)\n",
    "    l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)\n",
    "    if hasattr(l, \"block_sparse_moe\"):\n",
    "        l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate)\n",
    "\n",
    "p = sum(v.size for _, v in tree_flatten(model.parameters())) / 10**6\n",
    "print(f\"Total parameters {p:.3f}M\")\n",
    "p = sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6\n",
    "print(f\"Trainable parameters {p:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Light-weight wrapper to hold lines from a jsonl file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: Path, key: str = \"text\"):\n",
    "        if not path.exists():\n",
    "            self._data = None\n",
    "        else:\n",
    "            with open(path, \"r\") as fid:\n",
    "                self._data = [json.loads(l) for l in fid]\n",
    "        self._key = key\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self._data[idx][self._key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_folder: str, training: bool = False, validation: bool = False, testing: bool = False):\n",
    "    def load_and_check(name):\n",
    "        dataset_path = Path(data_folder) / f\"{name}.jsonl\"\n",
    "        try:\n",
    "            return Dataset(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to build dataset {dataset_path} ({e})\")\n",
    "            raise\n",
    "\n",
    "    names = (\"train-all-gcp\", \"valid-all-gcp\", \"test-all-gcp\")\n",
    "    train, valid, test = (load_and_check(n) for n in names)\n",
    "\n",
    "    if training and len(train) == 0:\n",
    "        raise ValueError(\n",
    "            \"Training set not found or empty. Must provide training set for fine-tuning.\"\n",
    "        )\n",
    "    if validation and len(valid) == 0:\n",
    "        raise ValueError(\n",
    "            \"Validation set not found or empty. Must provide validation set for fine-tuning.\"\n",
    "        )\n",
    "    if testing and len(test) == 0:\n",
    "        raise ValueError(\n",
    "            \"Test set not found or empty. Must provide test set for evaluation.\"\n",
    "        )\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training set: 150958, Validation set: 8386, Test set: 8386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'### Human: Merhaba babam dışarıdan ot toplayip koyuna vermiş akşama doğru ayaklarda titreme ve hiç ayağa. Kalkamiyrdu serum filan verildi şuan iyi ama eskisi gibi otlamiyor hem durup durup yerlere bakıyor ne önerirsiniz### Assistant: Bir müddet sonra düzelir hekim arkadaşın tavsiylerine uymaya devam edin.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading datasets\")\n",
    "train_set, valid_set, test_set = load(data_folder, training=True)\n",
    "print(f\"Training set: {len(train_set)}, Validation set: {len(valid_set)}, Test set: {len(test_set)}\")\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(dset, tokenizer, batch_size, train=False):\n",
    "    # Shuffle indices\n",
    "    while True:\n",
    "        indices = np.arange(len(dset))\n",
    "        if train:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        # Collect batches from dataset\n",
    "        for i in range(0, len(indices) - batch_size + 1, batch_size):\n",
    "            # Encode batch\n",
    "            batch = [tokenizer.encode(dset[indices[i + j]]) for j in range(batch_size)]\n",
    "            lengths = [len(x) for x in batch]\n",
    "\n",
    "            # Check if any sequence is longer than 2048 tokens\n",
    "            if max(lengths) > 2048:\n",
    "                print(\n",
    "                    \"[WARNING] Some sequences are longer than 2048 tokens. \"\n",
    "                    \"Consider pre-splitting your data to save memory.\"\n",
    "                )\n",
    "\n",
    "            # Pad to the max length\n",
    "            batch_arr = np.zeros((batch_size, max(lengths)), np.int32)\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                batch_arr[j, : lengths[j]] = batch[j]\n",
    "            batch = mx.array(batch_arr)\n",
    "            yield batch[:, :-1], batch[:, 1:], mx.array(lengths)\n",
    "\n",
    "        if not train:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, loss, tokenizer, batch_size, num_batches):\n",
    "    all_losses = []\n",
    "    ntokens = 0\n",
    "    for it, batch in zip(\n",
    "        range(num_batches),\n",
    "        iterate_batches(dataset, tokenizer, batch_size),\n",
    "    ):\n",
    "        losses, toks = loss(model, *batch)\n",
    "        all_losses.append((losses * toks).item())\n",
    "        ntokens += toks.item()\n",
    "\n",
    "    return np.sum(all_losses) / ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, val_set, optimizer, loss, tokenizer):\n",
    "    # Create value and grad function for loss\n",
    "    loss_value_and_grad = nn.value_and_grad(model, loss)\n",
    "\n",
    "    losses = []\n",
    "    n_tokens = 0\n",
    "\n",
    "    # Main training loop\n",
    "    start = time.perf_counter()\n",
    "    for it, batch in zip(\n",
    "        range(iters),\n",
    "        iterate_batches(train_set, tokenizer, batch_size, train=True),\n",
    "    ):\n",
    "        # Forward and backward pass\n",
    "        (lvalue, toks), grad = loss_value_and_grad(model, *batch)\n",
    "\n",
    "        # Model update\n",
    "        optimizer.update(model, grad)\n",
    "        mx.eval(model.parameters(), optimizer.state, lvalue)\n",
    "\n",
    "        # Record loss\n",
    "        losses.append(lvalue.item())\n",
    "        n_tokens += toks.item()\n",
    "\n",
    "        # Report training loss if needed\n",
    "        if (it + 1) % steps_per_report == 0:\n",
    "            train_loss = np.mean(losses)\n",
    "\n",
    "            stop = time.perf_counter()\n",
    "            print(\n",
    "                f\"Iter {it + 1}: Train loss {train_loss:.3f}, \"\n",
    "                f\"It/sec {steps_per_report / (stop - start):.3f}, \"\n",
    "                f\"Tokens/sec {float(n_tokens) / (stop - start):.3f}\"\n",
    "            )\n",
    "            losses = []\n",
    "            n_tokens = 0\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Report validation loss if needed\n",
    "        if it == 0 or (it + 1) % steps_per_eval == 0:\n",
    "            stop = time.perf_counter()\n",
    "            val_loss = evaluate(\n",
    "                model, val_set, loss, tokenizer, batch_size, val_batches\n",
    "            )\n",
    "            print(\n",
    "                f\"Iter {it + 1}: \"\n",
    "                f\"Val loss {val_loss:.3f}, \"\n",
    "                f\"Val took {(time.perf_counter() - stop):.3f}s\"\n",
    "            )\n",
    "\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Save adapter weights if needed\n",
    "        if (it + 1) % save_every == 0:\n",
    "            mx.savez(\n",
    "                adapter_file, **dict(tree_flatten(model.trainable_parameters()))\n",
    "            )\n",
    "            print(f\"Iter {it + 1}: Saved adapter weights to {adapter_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, inputs, targets, lengths):\n",
    "    # Run model on inputs\n",
    "    logits, _ = model(inputs)\n",
    "    logits = logits.astype(mx.float32)\n",
    "\n",
    "    # Mask padding tokens\n",
    "    length_mask = mx.arange(inputs.shape[1])[None, :] < lengths[:, None]\n",
    "\n",
    "    # Calculate the loss\n",
    "    ce = nn.losses.cross_entropy(logits, targets) * length_mask\n",
    "    ntoks = length_mask.sum()\n",
    "    ce = ce.sum() / ntoks\n",
    "    return ce, ntoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iter 1: Val loss 3.931, Val took 0.737s\n",
      "Iter 3: Train loss 3.956, It/sec 1.804, Tokens/sec 330.783\n",
      "Iter 6: Train loss 3.061, It/sec 0.707, Tokens/sec 258.210\n",
      "Iter 9: Train loss 3.105, It/sec 0.712, Tokens/sec 281.286\n",
      "Iter 12: Train loss 3.381, It/sec 1.013, Tokens/sec 341.839\n",
      "Iter 15: Train loss 3.244, It/sec 0.728, Tokens/sec 285.949\n",
      "Iter 15: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 18: Train loss 3.207, It/sec 0.777, Tokens/sec 278.354\n",
      "Iter 20: Val loss 3.181, Val took 0.666s\n",
      "Iter 21: Train loss 3.411, It/sec 2.156, Tokens/sec 572.834\n",
      "Iter 24: Train loss 3.362, It/sec 0.749, Tokens/sec 291.953\n",
      "Iter 27: Train loss 3.405, It/sec 0.539, Tokens/sec 279.775\n",
      "Iter 30: Train loss 2.972, It/sec 0.683, Tokens/sec 266.683\n",
      "Iter 30: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 33: Train loss 3.309, It/sec 0.892, Tokens/sec 292.851\n",
      "Iter 36: Train loss 3.293, It/sec 0.624, Tokens/sec 279.756\n",
      "Iter 39: Train loss 3.146, It/sec 1.048, Tokens/sec 289.190\n",
      "Iter 40: Val loss 3.201, Val took 0.828s\n",
      "Iter 42: Train loss 3.137, It/sec 1.140, Tokens/sec 480.526\n",
      "Iter 45: Train loss 3.145, It/sec 0.697, Tokens/sec 231.550\n",
      "Iter 45: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 48: Train loss 3.210, It/sec 0.561, Tokens/sec 262.643\n",
      "Iter 51: Train loss 3.142, It/sec 0.932, Tokens/sec 289.662\n",
      "Iter 54: Train loss 3.053, It/sec 0.610, Tokens/sec 273.237\n",
      "Iter 57: Train loss 3.282, It/sec 0.788, Tokens/sec 321.309\n",
      "Iter 60: Train loss 3.227, It/sec 0.871, Tokens/sec 280.140\n",
      "Iter 60: Val loss 3.254, Val took 0.659s\n",
      "Iter 60: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 63: Train loss 3.179, It/sec 0.982, Tokens/sec 319.457\n",
      "Iter 66: Train loss 2.916, It/sec 0.662, Tokens/sec 254.950\n",
      "Iter 69: Train loss 3.123, It/sec 0.719, Tokens/sec 272.929\n",
      "Iter 72: Train loss 3.098, It/sec 1.024, Tokens/sec 340.029\n",
      "Iter 75: Train loss 3.232, It/sec 0.518, Tokens/sec 268.917\n",
      "Iter 75: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 78: Train loss 3.044, It/sec 1.125, Tokens/sec 295.012\n",
      "Iter 80: Val loss 3.191, Val took 0.675s\n",
      "Iter 81: Train loss 3.277, It/sec 2.564, Tokens/sec 769.911\n",
      "Iter 84: Train loss 3.522, It/sec 0.784, Tokens/sec 263.042\n",
      "Iter 87: Train loss 3.044, It/sec 0.793, Tokens/sec 246.985\n",
      "Iter 90: Train loss 2.838, It/sec 0.325, Tokens/sec 263.937\n",
      "Iter 90: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 93: Train loss 3.050, It/sec 0.745, Tokens/sec 268.585\n",
      "Iter 96: Train loss 3.249, It/sec 0.773, Tokens/sec 274.399\n",
      "Iter 99: Train loss 3.141, It/sec 0.693, Tokens/sec 320.435\n",
      "Iter 100: Val loss 3.228, Val took 0.858s\n",
      "Iter 102: Train loss 3.146, It/sec 1.813, Tokens/sec 535.420\n",
      "Iter 105: Train loss 3.144, It/sec 0.512, Tokens/sec 287.869\n",
      "Iter 105: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 108: Train loss 3.152, It/sec 0.665, Tokens/sec 262.490\n",
      "Iter 111: Train loss 2.924, It/sec 0.592, Tokens/sec 252.541\n",
      "Iter 114: Train loss 3.082, It/sec 0.701, Tokens/sec 283.751\n",
      "Iter 117: Train loss 3.208, It/sec 0.934, Tokens/sec 228.003\n",
      "Iter 120: Train loss 3.190, It/sec 1.014, Tokens/sec 290.341\n",
      "Iter 120: Val loss 3.214, Val took 0.766s\n",
      "Iter 120: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 123: Train loss 3.118, It/sec 0.197, Tokens/sec 100.621\n",
      "Iter 126: Train loss 3.129, It/sec 0.234, Tokens/sec 114.444\n",
      "Iter 129: Train loss 3.374, It/sec 0.427, Tokens/sec 114.550\n",
      "Iter 132: Train loss 3.141, It/sec 0.730, Tokens/sec 292.811\n",
      "Iter 135: Train loss 3.084, It/sec 0.425, Tokens/sec 185.118\n",
      "Iter 135: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 138: Train loss 3.271, It/sec 0.420, Tokens/sec 176.888\n",
      "Iter 140: Val loss 3.166, Val took 0.964s\n",
      "Iter 141: Train loss 2.974, It/sec 2.151, Tokens/sec 733.638\n",
      "Iter 144: Train loss 3.208, It/sec 0.755, Tokens/sec 187.369\n",
      "Iter 147: Train loss 3.056, It/sec 0.858, Tokens/sec 233.934\n",
      "Iter 150: Train loss 2.815, It/sec 0.440, Tokens/sec 204.495\n",
      "Iter 150: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 153: Train loss 2.981, It/sec 0.461, Tokens/sec 228.723\n",
      "Iter 156: Train loss 3.066, It/sec 0.584, Tokens/sec 231.967\n",
      "Iter 159: Train loss 3.269, It/sec 0.491, Tokens/sec 196.251\n",
      "Iter 160: Val loss 3.157, Val took 1.126s\n",
      "Iter 162: Train loss 3.033, It/sec 1.827, Tokens/sec 707.558\n",
      "Iter 165: Train loss 3.090, It/sec 0.536, Tokens/sec 196.919\n",
      "Iter 165: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 168: Train loss 3.131, It/sec 0.614, Tokens/sec 196.595\n",
      "Iter 171: Train loss 2.824, It/sec 0.466, Tokens/sec 186.565\n",
      "Iter 174: Train loss 2.907, It/sec 0.566, Tokens/sec 201.184\n",
      "Iter 177: Train loss 3.073, It/sec 0.641, Tokens/sec 210.540\n",
      "Iter 180: Train loss 3.182, It/sec 0.314, Tokens/sec 184.205\n",
      "Iter 180: Val loss 3.180, Val took 1.004s\n",
      "Iter 180: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 183: Train loss 2.959, It/sec 0.917, Tokens/sec 234.405\n",
      "Iter 186: Train loss 2.975, It/sec 0.376, Tokens/sec 204.168\n",
      "Iter 189: Train loss 3.126, It/sec 0.625, Tokens/sec 208.484\n",
      "Iter 192: Train loss 3.135, It/sec 0.416, Tokens/sec 198.888\n",
      "Iter 195: Train loss 2.996, It/sec 0.610, Tokens/sec 208.419\n",
      "Iter 195: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 198: Train loss 3.027, It/sec 0.536, Tokens/sec 220.824\n",
      "Iter 200: Val loss 3.167, Val took 1.023s\n",
      "Iter 201: Train loss 2.750, It/sec 2.639, Tokens/sec 932.334\n",
      "Iter 204: Train loss 3.359, It/sec 0.822, Tokens/sec 205.869\n",
      "Iter 207: Train loss 3.062, It/sec 0.778, Tokens/sec 245.648\n",
      "Iter 210: Train loss 2.983, It/sec 0.782, Tokens/sec 219.990\n",
      "Iter 210: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 213: Train loss 3.022, It/sec 0.552, Tokens/sec 187.048\n",
      "Iter 216: Train loss 3.479, It/sec 0.675, Tokens/sec 191.043\n",
      "Iter 219: Train loss 2.864, It/sec 0.643, Tokens/sec 180.156\n",
      "Iter 220: Val loss 3.171, Val took 0.998s\n",
      "Iter 222: Train loss 3.099, It/sec 0.686, Tokens/sec 301.817\n",
      "Iter 225: Train loss 2.928, It/sec 0.448, Tokens/sec 180.035\n",
      "Iter 225: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 228: Train loss 3.089, It/sec 0.659, Tokens/sec 221.517\n",
      "Iter 231: Train loss 2.965, It/sec 0.468, Tokens/sec 197.802\n",
      "Iter 234: Train loss 2.795, It/sec 0.708, Tokens/sec 222.061\n",
      "Iter 237: Train loss 3.152, It/sec 0.520, Tokens/sec 205.947\n",
      "Iter 240: Train loss 2.875, It/sec 0.671, Tokens/sec 198.313\n",
      "Iter 240: Val loss 3.165, Val took 1.080s\n",
      "Iter 240: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 243: Train loss 3.440, It/sec 0.566, Tokens/sec 207.425\n",
      "Iter 246: Train loss 3.085, It/sec 0.425, Tokens/sec 192.676\n",
      "Iter 249: Train loss 3.069, It/sec 0.529, Tokens/sec 205.376\n",
      "Iter 252: Train loss 3.122, It/sec 0.654, Tokens/sec 155.791\n",
      "Iter 255: Train loss 2.927, It/sec 0.603, Tokens/sec 213.115\n",
      "Iter 255: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 258: Train loss 3.150, It/sec 0.672, Tokens/sec 192.406\n",
      "Iter 260: Val loss 3.170, Val took 1.167s\n",
      "Iter 261: Train loss 3.151, It/sec 2.208, Tokens/sec 983.133\n",
      "Iter 264: Train loss 2.994, It/sec 0.481, Tokens/sec 208.053\n",
      "Iter 267: Train loss 3.084, It/sec 0.653, Tokens/sec 185.671\n",
      "Iter 270: Train loss 3.165, It/sec 0.726, Tokens/sec 223.038\n",
      "Iter 270: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 273: Train loss 2.944, It/sec 0.602, Tokens/sec 212.126\n",
      "Iter 276: Train loss 2.898, It/sec 0.625, Tokens/sec 229.646\n",
      "Iter 279: Train loss 2.956, It/sec 0.309, Tokens/sec 171.247\n",
      "Iter 280: Val loss 3.185, Val took 0.980s\n",
      "Iter 282: Train loss 3.125, It/sec 0.727, Tokens/sec 332.151\n",
      "Iter 285: Train loss 2.770, It/sec 0.662, Tokens/sec 203.501\n",
      "Iter 285: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n",
      "Iter 288: Train loss 3.209, It/sec 0.547, Tokens/sec 236.671\n",
      "Iter 291: Train loss 3.248, It/sec 0.645, Tokens/sec 232.809\n",
      "Iter 294: Train loss 2.858, It/sec 0.427, Tokens/sec 235.962\n",
      "Iter 297: Train loss 2.773, It/sec 0.671, Tokens/sec 231.429\n",
      "Iter 300: Train loss 2.715, It/sec 0.597, Tokens/sec 182.948\n",
      "Iter 300: Val loss 3.152, Val took 0.919s\n",
      "Iter 300: Saved adapter weights to adapters/20240616-202312-adapters-all.npz.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "opt = optim.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Train model\n",
    "train(model, train_set, valid_set, opt, loss, tokenizer)\n",
    "\n",
    "# Save adapter weights\n",
    "mx.savez(adapter_file, **dict(tree_flatten(model.trainable_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merhaba sol femur alt uç kırığı ve kalça kırığı çivisi takılmıştı fazlasıyla rahatsız ediyor aldiricam sonrasında ne zamana kadar normal yasantima dönerim?teşekkürler.### Assistant: Merhaba Ameliyatla kaynama şansı var. Bu durumda değişiklikler olur. Fakat ortopedi uzmanına muayene olmanız daha uygun olur. İyi günler.<unk> AKKOR İyi günler. Geçmiş olsun. Geçmiş olsun.,.<unk> AKKOR Geçmiş olsun. Dr. Ali Akben Akben ile ilgili bu sorununuzu inceleyeceğim. Dr Ali Akben Akben Uzm. Dr Ali Akben Akben ile iletişime geçmeniz daha uygun olur. İyi günler. Dr Ali Akben Akben www.draliakkben.com Not: Bu mesajın bir kopyasını saklı tutmaya hakkım vardır. Dr Ali Akben Akben www.draliakkben.com Not: Bu mesajın bir kopyasını saklı tutmaya hakkım vardır. Dr Ali Akben Akben www.draliakkben.com Not: Bu mesaj\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate(model, example_prompt, tokenizer, temp, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"generated_text_after_finetuning\"] = generated_text\n",
    "result[\"temp\"] = temp\n",
    "result[\"max_tokens\"] = max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result to json file timestamped in utf-8\n",
    "result_file = f\"results/{time.strftime('%Y%m%d-%H%M%S')}-result.json\"\n",
    "with open(result_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
