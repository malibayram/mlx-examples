{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "import utils as lora_utils\n",
    "from mlx.utils import tree_flatten\n",
    "from models import LoRALinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "add_eos_token = True\n",
    "data_folder = \"my-data-text\"\n",
    "lora_layers = 4\n",
    "batch_size = 1\n",
    "iters = 1000\n",
    "steps_per_report = 10\n",
    "steps_per_eval = 200\n",
    "val_batches = 25\n",
    "learning_rate = 1e-5\n",
    "seed = 0\n",
    "save_every = 100\n",
    "adapter_file = \"adapters.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 127804.28it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Building tokenizer_config\n",
    "tokenizer_config = {\"add_eos_token\": add_eos_token}\n",
    "\n",
    "print(\"Loading pretrained model\")\n",
    "model, tokenizer, _ = lora_utils.load(model, tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 8030.687M\n",
      "Trainable parameters 0.426M\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers other than LORA linears\n",
    "model.freeze()\n",
    "for l in model.model.layers[len(model.model.layers) - lora_layers :]:\n",
    "    l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)\n",
    "    l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)\n",
    "    if hasattr(l, \"block_sparse_moe\"):\n",
    "        l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate)\n",
    "\n",
    "p = sum(v.size for _, v in tree_flatten(model.parameters())) / 10**6\n",
    "print(f\"Total parameters {p:.3f}M\")\n",
    "p = sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6\n",
    "print(f\"Trainable parameters {p:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Light-weight wrapper to hold lines from a jsonl file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: Path, key: str = \"text\"):\n",
    "        if not path.exists():\n",
    "            self._data = None\n",
    "        else:\n",
    "            with open(path, \"r\") as fid:\n",
    "                self._data = [json.loads(l) for l in fid]\n",
    "        self._key = key\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self._data[idx][self._key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_folder: str, training: bool = False, validation: bool = False, testing: bool = False):\n",
    "    def load_and_check(name):\n",
    "        dataset_path = Path(data_folder) / f\"{name}.jsonl\"\n",
    "        try:\n",
    "            return Dataset(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to build dataset {dataset_path} ({e})\")\n",
    "            raise\n",
    "\n",
    "    names = (\"train\", \"valid\", \"test\")\n",
    "    train, valid, test = (load_and_check(n) for n in names)\n",
    "\n",
    "    if training and len(train) == 0:\n",
    "        raise ValueError(\n",
    "            \"Training set not found or empty. Must provide training set for fine-tuning.\"\n",
    "        )\n",
    "    if validation and len(valid) == 0:\n",
    "        raise ValueError(\n",
    "            \"Validation set not found or empty. Must provide validation set for fine-tuning.\"\n",
    "        )\n",
    "    if testing and len(test) == 0:\n",
    "        raise ValueError(\n",
    "            \"Test set not found or empty. Must provide test set for evaluation.\"\n",
    "        )\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training set: 100, Validation set: 25, Test set: 25\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets\")\n",
    "train_set, valid_set, test_set = load(data_folder, training=True)\n",
    "print(f\"Training set: {len(train_set)}, Validation set: {len(valid_set)}, Test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 1195, 10796, 64, 111984]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"merhaba dünya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(dset, tokenizer, batch_size, train=False):\n",
    "    # Shuffle indices\n",
    "    while True:\n",
    "        indices = np.arange(len(dset))\n",
    "        if train:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        # Collect batches from dataset\n",
    "        for i in range(0, len(indices) - batch_size + 1, batch_size):\n",
    "            # Encode batch\n",
    "            batch = [tokenizer.encode(dset[indices[i + j]]) for j in range(batch_size)]\n",
    "            lengths = [len(x) for x in batch]\n",
    "\n",
    "            # Check if any sequence is longer than 2048 tokens\n",
    "            if max(lengths) > 2048:\n",
    "                print(\n",
    "                    \"[WARNING] Some sequences are longer than 2048 tokens. \"\n",
    "                    \"Consider pre-splitting your data to save memory.\"\n",
    "                )\n",
    "\n",
    "            # Pad to the max length\n",
    "            batch_arr = np.zeros((batch_size, max(lengths)), np.int32)\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                batch_arr[j, : lengths[j]] = batch[j]\n",
    "            batch = mx.array(batch_arr)\n",
    "            yield batch[:, :-1], batch[:, 1:], mx.array(lengths)\n",
    "\n",
    "        if not train:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, loss, tokenizer, batch_size, num_batches):\n",
    "    all_losses = []\n",
    "    ntokens = 0\n",
    "    for it, batch in zip(\n",
    "        range(num_batches),\n",
    "        iterate_batches(dataset, tokenizer, batch_size),\n",
    "    ):\n",
    "        losses, toks = loss(model, *batch)\n",
    "        all_losses.append((losses * toks).item())\n",
    "        ntokens += toks.item()\n",
    "\n",
    "    return np.sum(all_losses) / ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, val_set, optimizer, loss, tokenizer):\n",
    "    # Create value and grad function for loss\n",
    "    loss_value_and_grad = nn.value_and_grad(model, loss)\n",
    "\n",
    "    losses = []\n",
    "    n_tokens = 0\n",
    "\n",
    "    # Main training loop\n",
    "    start = time.perf_counter()\n",
    "    for it, batch in zip(\n",
    "        range(iters),\n",
    "        iterate_batches(train_set, tokenizer, batch_size, train=True),\n",
    "    ):\n",
    "        # Forward and backward pass\n",
    "        (lvalue, toks), grad = loss_value_and_grad(model, *batch)\n",
    "\n",
    "        # Model update\n",
    "        optimizer.update(model, grad)\n",
    "        mx.eval(model.parameters(), optimizer.state, lvalue)\n",
    "\n",
    "        # Record loss\n",
    "        losses.append(lvalue.item())\n",
    "        n_tokens += toks.item()\n",
    "\n",
    "        # Report training loss if needed\n",
    "        if (it + 1) % steps_per_report == 0:\n",
    "            train_loss = np.mean(losses)\n",
    "\n",
    "            stop = time.perf_counter()\n",
    "            print(\n",
    "                f\"Iter {it + 1}: Train loss {train_loss:.3f}, \"\n",
    "                f\"It/sec {steps_per_report / (stop - start):.3f}, \"\n",
    "                f\"Tokens/sec {float(n_tokens) / (stop - start):.3f}\"\n",
    "            )\n",
    "            losses = []\n",
    "            n_tokens = 0\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Report validation loss if needed\n",
    "        if it == 0 or (it + 1) % steps_per_eval == 0:\n",
    "            stop = time.perf_counter()\n",
    "            val_loss = evaluate(\n",
    "                model, val_set, loss, tokenizer, batch_size, val_batches\n",
    "            )\n",
    "            print(\n",
    "                f\"Iter {it + 1}: \"\n",
    "                f\"Val loss {val_loss:.3f}, \"\n",
    "                f\"Val took {(time.perf_counter() - stop):.3f}s\"\n",
    "            )\n",
    "\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Save adapter weights if needed\n",
    "        if (it + 1) % save_every == 0:\n",
    "            mx.savez(\n",
    "                adapter_file, **dict(tree_flatten(model.trainable_parameters()))\n",
    "            )\n",
    "            print(f\"Iter {it + 1}: Saved adapter weights to {adapter_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, inputs, targets, lengths):\n",
    "    # Run model on inputs\n",
    "    logits, _ = model(inputs)\n",
    "    logits = logits.astype(mx.float32)\n",
    "\n",
    "    # Mask padding tokens\n",
    "    length_mask = mx.arange(inputs.shape[1])[None, :] < lengths[:, None]\n",
    "\n",
    "    # Calculate the loss\n",
    "    ce = nn.losses.cross_entropy(logits, targets) * length_mask\n",
    "    ntoks = length_mask.sum()\n",
    "    ce = ce.sum() / ntoks\n",
    "    return ce, ntoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iter 1: Val loss 3.047, Val took 7.306s\n",
      "Iter 10: Train loss 3.087, It/sec 3.328, Tokens/sec 305.165\n",
      "Iter 20: Train loss 2.937, It/sec 3.007, Tokens/sec 261.934\n",
      "Iter 30: Train loss 2.469, It/sec 2.931, Tokens/sec 269.346\n",
      "Iter 40: Train loss 1.982, It/sec 2.977, Tokens/sec 279.528\n",
      "Iter 50: Train loss 1.584, It/sec 3.032, Tokens/sec 275.888\n",
      "Iter 60: Train loss 1.468, It/sec 3.097, Tokens/sec 278.145\n",
      "Iter 70: Train loss 1.246, It/sec 2.926, Tokens/sec 279.427\n",
      "Iter 80: Train loss 1.176, It/sec 3.052, Tokens/sec 279.221\n",
      "Iter 90: Train loss 1.138, It/sec 2.965, Tokens/sec 274.283\n",
      "Iter 100: Train loss 1.070, It/sec 3.128, Tokens/sec 267.463\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "Iter 110: Train loss 1.018, It/sec 3.145, Tokens/sec 273.960\n",
      "Iter 120: Train loss 1.126, It/sec 3.075, Tokens/sec 281.670\n",
      "Iter 130: Train loss 1.099, It/sec 3.045, Tokens/sec 289.273\n",
      "Iter 140: Train loss 1.153, It/sec 3.048, Tokens/sec 282.549\n",
      "Iter 150: Train loss 1.094, It/sec 3.081, Tokens/sec 283.150\n",
      "Iter 160: Train loss 1.020, It/sec 3.209, Tokens/sec 271.817\n",
      "Iter 170: Train loss 0.982, It/sec 3.016, Tokens/sec 274.160\n",
      "Iter 180: Train loss 0.961, It/sec 3.229, Tokens/sec 286.111\n",
      "Iter 190: Train loss 1.128, It/sec 3.021, Tokens/sec 290.958\n",
      "Iter 200: Train loss 1.014, It/sec 3.119, Tokens/sec 285.683\n",
      "Iter 200: Val loss 1.070, Val took 6.817s\n",
      "Iter 200: Saved adapter weights to adapters.npz.\n",
      "Iter 210: Train loss 0.887, It/sec 3.185, Tokens/sec 279.329\n",
      "Iter 220: Train loss 0.970, It/sec 3.147, Tokens/sec 279.785\n",
      "Iter 230: Train loss 1.070, It/sec 3.010, Tokens/sec 295.272\n",
      "Iter 240: Train loss 1.034, It/sec 2.902, Tokens/sec 296.854\n",
      "Iter 250: Train loss 1.047, It/sec 3.268, Tokens/sec 283.959\n",
      "Iter 260: Train loss 1.035, It/sec 3.213, Tokens/sec 284.650\n",
      "Iter 270: Train loss 0.959, It/sec 3.240, Tokens/sec 282.513\n",
      "Iter 280: Train loss 0.864, It/sec 3.074, Tokens/sec 270.523\n",
      "Iter 290: Train loss 0.902, It/sec 2.855, Tokens/sec 283.741\n",
      "Iter 300: Train loss 1.002, It/sec 3.052, Tokens/sec 254.203\n",
      "Iter 300: Saved adapter weights to adapters.npz.\n",
      "Iter 310: Train loss 0.963, It/sec 2.956, Tokens/sec 278.726\n",
      "Iter 320: Train loss 0.965, It/sec 2.915, Tokens/sec 281.288\n",
      "Iter 330: Train loss 0.911, It/sec 3.064, Tokens/sec 274.832\n",
      "Iter 340: Train loss 0.880, It/sec 3.156, Tokens/sec 260.346\n",
      "Iter 350: Train loss 1.003, It/sec 2.992, Tokens/sec 301.919\n",
      "Iter 360: Train loss 0.920, It/sec 3.201, Tokens/sec 283.609\n",
      "Iter 370: Train loss 0.912, It/sec 3.171, Tokens/sec 288.902\n",
      "Iter 380: Train loss 0.894, It/sec 3.054, Tokens/sec 290.709\n",
      "Iter 390: Train loss 0.822, It/sec 3.337, Tokens/sec 270.624\n",
      "Iter 400: Train loss 0.893, It/sec 3.134, Tokens/sec 283.667\n",
      "Iter 400: Val loss 1.008, Val took 6.801s\n",
      "Iter 400: Saved adapter weights to adapters.npz.\n",
      "Iter 410: Train loss 0.736, It/sec 3.176, Tokens/sec 282.046\n",
      "Iter 420: Train loss 0.863, It/sec 3.135, Tokens/sec 284.619\n",
      "Iter 430: Train loss 0.992, It/sec 3.024, Tokens/sec 295.137\n",
      "Iter 440: Train loss 0.904, It/sec 3.054, Tokens/sec 287.699\n",
      "Iter 450: Train loss 0.906, It/sec 2.986, Tokens/sec 300.087\n",
      "Iter 460: Train loss 0.813, It/sec 3.252, Tokens/sec 275.731\n",
      "Iter 470: Train loss 1.006, It/sec 3.140, Tokens/sec 293.875\n",
      "Iter 480: Train loss 0.843, It/sec 3.137, Tokens/sec 290.811\n",
      "Iter 490: Train loss 0.788, It/sec 3.265, Tokens/sec 271.007\n",
      "Iter 500: Train loss 0.880, It/sec 3.250, Tokens/sec 274.326\n",
      "Iter 500: Saved adapter weights to adapters.npz.\n",
      "Iter 510: Train loss 0.911, It/sec 3.169, Tokens/sec 277.296\n",
      "Iter 520: Train loss 0.856, It/sec 3.201, Tokens/sec 285.193\n",
      "Iter 530: Train loss 0.824, It/sec 3.193, Tokens/sec 275.252\n",
      "Iter 540: Train loss 0.808, It/sec 3.173, Tokens/sec 280.518\n",
      "Iter 550: Train loss 0.713, It/sec 3.109, Tokens/sec 284.777\n",
      "Iter 560: Train loss 0.797, It/sec 3.098, Tokens/sec 280.994\n",
      "Iter 570: Train loss 0.819, It/sec 3.115, Tokens/sec 293.740\n",
      "Iter 580: Train loss 0.966, It/sec 2.865, Tokens/sec 291.363\n",
      "Iter 590: Train loss 0.896, It/sec 3.127, Tokens/sec 292.076\n",
      "Iter 600: Train loss 0.826, It/sec 3.135, Tokens/sec 274.349\n",
      "Iter 600: Val loss 0.997, Val took 7.362s\n",
      "Iter 600: Saved adapter weights to adapters.npz.\n",
      "Iter 610: Train loss 0.757, It/sec 2.923, Tokens/sec 267.151\n",
      "Iter 620: Train loss 0.669, It/sec 3.054, Tokens/sec 244.641\n",
      "Iter 630: Train loss 0.904, It/sec 2.809, Tokens/sec 270.776\n",
      "Iter 640: Train loss 0.824, It/sec 2.886, Tokens/sec 261.467\n",
      "Iter 650: Train loss 0.765, It/sec 2.889, Tokens/sec 251.061\n",
      "Iter 660: Train loss 0.901, It/sec 2.765, Tokens/sec 259.101\n",
      "Iter 670: Train loss 0.831, It/sec 2.818, Tokens/sec 251.896\n",
      "Iter 680: Train loss 0.794, It/sec 2.597, Tokens/sec 252.912\n",
      "Iter 690: Train loss 0.830, It/sec 2.770, Tokens/sec 252.864\n",
      "Iter 700: Train loss 0.875, It/sec 2.691, Tokens/sec 250.803\n",
      "Iter 700: Saved adapter weights to adapters.npz.\n",
      "Iter 710: Train loss 0.752, It/sec 2.810, Tokens/sec 244.166\n",
      "Iter 720: Train loss 0.941, It/sec 2.648, Tokens/sec 259.464\n",
      "Iter 730: Train loss 0.745, It/sec 2.800, Tokens/sec 242.233\n",
      "Iter 740: Train loss 0.788, It/sec 2.627, Tokens/sec 239.850\n",
      "Iter 750: Train loss 0.720, It/sec 2.744, Tokens/sec 241.747\n",
      "Iter 760: Train loss 0.801, It/sec 2.614, Tokens/sec 248.350\n",
      "Iter 770: Train loss 0.780, It/sec 2.648, Tokens/sec 241.778\n",
      "Iter 780: Train loss 0.815, It/sec 2.626, Tokens/sec 238.747\n",
      "Iter 790: Train loss 0.759, It/sec 2.723, Tokens/sec 240.167\n",
      "Iter 800: Train loss 0.816, It/sec 2.644, Tokens/sec 249.050\n",
      "Iter 800: Val loss 0.997, Val took 8.111s\n",
      "Iter 800: Saved adapter weights to adapters.npz.\n",
      "Iter 810: Train loss 0.758, It/sec 2.675, Tokens/sec 241.022\n",
      "Iter 820: Train loss 0.607, It/sec 2.638, Tokens/sec 227.144\n",
      "Iter 830: Train loss 0.775, It/sec 2.529, Tokens/sec 237.179\n",
      "Iter 840: Train loss 0.789, It/sec 2.589, Tokens/sec 240.238\n",
      "Iter 850: Train loss 0.826, It/sec 2.647, Tokens/sec 247.717\n",
      "Iter 860: Train loss 0.721, It/sec 2.720, Tokens/sec 237.986\n",
      "Iter 870: Train loss 0.760, It/sec 2.624, Tokens/sec 234.317\n",
      "Iter 880: Train loss 0.862, It/sec 2.568, Tokens/sec 242.116\n",
      "Iter 890: Train loss 0.766, It/sec 2.654, Tokens/sec 242.885\n",
      "Iter 900: Train loss 0.856, It/sec 2.691, Tokens/sec 245.984\n",
      "Iter 900: Saved adapter weights to adapters.npz.\n",
      "Iter 910: Train loss 0.743, It/sec 2.677, Tokens/sec 236.395\n",
      "Iter 920: Train loss 0.757, It/sec 2.628, Tokens/sec 239.416\n",
      "Iter 930: Train loss 0.657, It/sec 2.579, Tokens/sec 233.363\n",
      "Iter 940: Train loss 0.737, It/sec 2.563, Tokens/sec 245.760\n",
      "Iter 950: Train loss 0.831, It/sec 2.562, Tokens/sec 240.338\n",
      "Iter 960: Train loss 0.756, It/sec 2.715, Tokens/sec 232.119\n",
      "Iter 970: Train loss 0.783, It/sec 2.626, Tokens/sec 248.113\n",
      "Iter 980: Train loss 0.787, It/sec 2.703, Tokens/sec 246.474\n",
      "Iter 990: Train loss 0.781, It/sec 2.657, Tokens/sec 240.973\n",
      "Iter 1000: Train loss 0.691, It/sec 2.711, Tokens/sec 240.965\n",
      "Iter 1000: Val loss 1.008, Val took 7.957s\n",
      "Iter 1000: Saved adapter weights to adapters.npz.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "opt = optim.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Train model\n",
    "train(model, train_set, valid_set, opt, loss, tokenizer)\n",
    "\n",
    "# Save adapter weights\n",
    "mx.savez(adapter_file, **dict(tree_flatten(model.trainable_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, tokenizer, temp, max_tokens):\n",
    "    print(prompt, end=\"\", flush=True)\n",
    "\n",
    "    prompt = mx.array(tokenizer.encode(prompt))\n",
    "\n",
    "    tokens = []\n",
    "    skip = 0\n",
    "    for token, n in zip(\n",
    "        lora_utils.generate(prompt, model, temp),\n",
    "        range(max_tokens),\n",
    "    ):\n",
    "        if token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        tokens.append(token.item())\n",
    "        s = tokenizer.decode(tokens)\n",
    "        if len(s) - skip > 1:\n",
    "            print(s[skip:-1], end=\"\", flush=True)\n",
    "            skip = len(s) - 1\n",
    "    print(tokenizer.decode(tokens)[skip:], flush=True)\n",
    "    print(\"=\" * 10)\n",
    "    if len(tokens) == 0:\n",
    "        print(\"No tokens generated for this prompt\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merhaba dünya! welcome to my blog!\n",
      "i'm a turkish student, and i'm here to share my thoughts, experiences and adventures with you.\n",
      "i'm a coffee lover, a bookworm, and a music enthusiast. i'm always up for a good conversation, a cup of coffee, or a game of chess.\n",
      "in this blog, you can expect to find posts about my daily life, my studies, my travels, and my hobbies. i'll be sharing my thoughts on various topics, from science\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "temp = 0.7\n",
    "max_tokens = 100\n",
    "generate(model, \"merhaba dünya\", tokenizer, temp, max_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
