{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mab/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize tokenizer and model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save the tokenizer and model locally (optional)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./gpt2-turkish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_utils.py:3306\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3291\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3292\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3293\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3294\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3305\u001b[0m     }\n\u001b[0;32m-> 3306\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3308\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3309\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3311\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:1492\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1492\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/huggingface_hub/file_download.py:535\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    533\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    537\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Save the tokenizer and model locally (optional)\n",
    "tokenizer.save_pretrained('./gpt2-turkish')\n",
    "model.save_pretrained('./gpt2-turkish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "import utils as lora_utils\n",
    "from mlx.utils import tree_flatten\n",
    "from models import LoRALinear\n",
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "data_folder = \"../../veri/doktorsitesi/my-data-text\"\n",
    "lora_layers = 4\n",
    "batch_size = 8\n",
    "iters = 100\n",
    "steps_per_report = 2\n",
    "steps_per_eval = 20\n",
    "val_batches = 8\n",
    "learning_rate = 1e-4\n",
    "seed = 0\n",
    "save_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240612-184324-adapters-Qwen2-0.5B-Instruct.npz'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_file = f\"{time.strftime('%Y%m%d-%H%M%S')}-adapters-{model_path.split('/')[-1]}.npz\"\n",
    "adapter_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer, _ = lora_utils.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 103017.99it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained model\")\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"Hasta:Ankilozan Spondilit ve omurilik ve göğüs kafesi kemikleri birbirine girdi ve kaynamış biz bu hastalığın tedavisi var mı? Lütfen türkçe cevap ver, tercüme etme\"\n",
    "verbose = True\n",
    "top_p = 0.8\n",
    "temperature = 0.7\n",
    "repetition_penalty = 1.05\n",
    "max_tokens = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: Hasta:Ankilozan Spondilit ve omurilik ve göğüs kafesi kemikleri birbirine girdi ve kaynamış biz bu hastalığın tedavisi var mı? Lütfen türkçe cevap ver, tercüme etme\n",
      " veya sorunun alınıyor.\n",
      "\n",
      "Translate to English\n",
      "\n",
      "He said that there was a risk of the two drugs being combined in the future. He asked if they were going to be used together in the future.\n",
      "\n",
      "Sorry, but I can't assist with that.\n",
      "==========\n",
      "Prompt: 74.852 tokens-per-sec\n",
      "Generation: 87.941 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "# response = generate(model, tokenizer, prompt=example_prompt, verbose=True, top_p=0.8, temp=0.7, repetition_penalty=1.05, max_tokens=512)\n",
    "response = generate(model, tokenizer, prompt=example_prompt, verbose=True, top_p=top_p, temp=temperature, repetition_penalty=repetition_penalty, max_tokens=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers.0): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.1): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.2): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.3): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.4): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.5): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.6): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.7): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.8): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.9): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.10): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.11): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.12): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.13): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.14): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.15): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.16): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.17): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.18): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.19): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.20): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.21): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.22): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (layers.23): TransformerBlock(\n",
       "      (self_attn): Attention(\n",
       "        (q_proj): Linear(input_dims=896, output_dims=896, bias=True)\n",
       "        (k_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (v_proj): Linear(input_dims=896, output_dims=128, bias=True)\n",
       "        (o_proj): Linear(input_dims=896, output_dims=896, bias=False)\n",
       "        (rope): RoPE(64, traditional=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (gate_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "        (down_proj): Linear(input_dims=4864, output_dims=896, bias=False)\n",
       "        (up_proj): Linear(input_dims=896, output_dims=4864, bias=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm(896, eps=1e-06)\n",
       "      (post_attention_layernorm): RMSNorm(896, eps=1e-06)\n",
       "    )\n",
       "    (norm): RMSNorm(896, eps=1e-06)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze all layers other than LORA linears\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters 494.123M\n",
      "Trainable parameters 0.090M\n"
     ]
    }
   ],
   "source": [
    "for l in model.model.layers[len(model.model.layers) - lora_layers :]:\n",
    "    l.self_attn.q_proj = LoRALinear.from_linear(l.self_attn.q_proj)\n",
    "    l.self_attn.v_proj = LoRALinear.from_linear(l.self_attn.v_proj)\n",
    "    if hasattr(l, \"block_sparse_moe\"):\n",
    "        l.block_sparse_moe.gate = LoRALinear.from_linear(l.block_sparse_moe.gate)\n",
    "\n",
    "p = sum(v.size for _, v in tree_flatten(model.parameters())) / 10**6\n",
    "print(f\"Total parameters {p:.3f}M\")\n",
    "p = sum(v.size for _, v in tree_flatten(model.trainable_parameters())) / 10**6\n",
    "print(f\"Trainable parameters {p:.3f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Light-weight wrapper to hold lines from a jsonl file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: Path, key: str = \"text\"):\n",
    "        if not path.exists():\n",
    "            self._data = None\n",
    "        else:\n",
    "            with open(path, \"r\") as fid:\n",
    "                self._data = [json.loads(l) for l in fid]\n",
    "        self._key = key\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self._data[idx][self._key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_folder: str, training: bool = False, validation: bool = False, testing: bool = False):\n",
    "    def load_and_check(name):\n",
    "        dataset_path = Path(data_folder) / f\"{name}.jsonl\"\n",
    "        try:\n",
    "            return Dataset(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to build dataset {dataset_path} ({e})\")\n",
    "            raise\n",
    "\n",
    "    names = (\"train-5000\", \"valid-5000\", \"test-5000\")\n",
    "    train, valid, test = (load_and_check(n) for n in names)\n",
    "\n",
    "    if training and len(train) == 0:\n",
    "        raise ValueError(\n",
    "            \"Training set not found or empty. Must provide training set for fine-tuning.\"\n",
    "        )\n",
    "    if validation and len(valid) == 0:\n",
    "        raise ValueError(\n",
    "            \"Validation set not found or empty. Must provide validation set for fine-tuning.\"\n",
    "        )\n",
    "    if testing and len(test) == 0:\n",
    "        raise ValueError(\n",
    "            \"Test set not found or empty. Must provide test set for evaluation.\"\n",
    "        )\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Training set: 4000, Validation set: 500, Test set: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets\")\n",
    "train_set, valid_set, test_set = load(data_folder, training=True)\n",
    "print(f\"Training set: {len(train_set)}, Validation set: {len(valid_set)}, Test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(dset, tokenizer, batch_size, train=False):\n",
    "    # Shuffle indices\n",
    "    while True:\n",
    "        indices = np.arange(len(dset))\n",
    "        if train:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        # Collect batches from dataset\n",
    "        for i in range(0, len(indices) - batch_size + 1, batch_size):\n",
    "            # Encode batch\n",
    "            batch = [tokenizer.encode(dset[indices[i + j]]) for j in range(batch_size)]\n",
    "            lengths = [len(x) for x in batch]\n",
    "\n",
    "            # Check if any sequence is longer than 2048 tokens\n",
    "            if max(lengths) > 2048:\n",
    "                print(\n",
    "                    \"[WARNING] Some sequences are longer than 2048 tokens. \"\n",
    "                    \"Consider pre-splitting your data to save memory.\"\n",
    "                )\n",
    "\n",
    "            # Pad to the max length\n",
    "            batch_arr = np.zeros((batch_size, max(lengths)), np.int32)\n",
    "\n",
    "            for j in range(batch_size):\n",
    "                batch_arr[j, : lengths[j]] = batch[j]\n",
    "            batch = mx.array(batch_arr)\n",
    "            yield batch[:, :-1], batch[:, 1:], mx.array(lengths)\n",
    "\n",
    "        if not train:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, loss, tokenizer, batch_size, num_batches):\n",
    "    all_losses = []\n",
    "    ntokens = 0\n",
    "    for it, batch in zip(\n",
    "        range(num_batches),\n",
    "        iterate_batches(dataset, tokenizer, batch_size),\n",
    "    ):\n",
    "        losses, toks = loss(model, *batch)\n",
    "        all_losses.append((losses * toks).item())\n",
    "        ntokens += toks.item()\n",
    "\n",
    "    return np.sum(all_losses) / ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_set, val_set, optimizer, loss, tokenizer):\n",
    "    # Create value and grad function for loss\n",
    "    loss_value_and_grad = nn.value_and_grad(model, loss)\n",
    "\n",
    "    losses = []\n",
    "    n_tokens = 0\n",
    "\n",
    "    # Main training loop\n",
    "    start = time.perf_counter()\n",
    "    for it, batch in zip(\n",
    "        range(iters),\n",
    "        iterate_batches(train_set, tokenizer, batch_size, train=True),\n",
    "    ):\n",
    "        # Forward and backward pass\n",
    "        (lvalue, toks), grad = loss_value_and_grad(model, *batch)\n",
    "\n",
    "        # Model update\n",
    "        optimizer.update(model, grad)\n",
    "        mx.eval(model.parameters(), optimizer.state, lvalue)\n",
    "\n",
    "        # Record loss\n",
    "        losses.append(lvalue.item())\n",
    "        n_tokens += toks.item()\n",
    "\n",
    "        # Report training loss if needed\n",
    "        if (it + 1) % steps_per_report == 0:\n",
    "            train_loss = np.mean(losses)\n",
    "\n",
    "            stop = time.perf_counter()\n",
    "            print(\n",
    "                f\"Iter {it + 1}: Train loss {train_loss:.3f}, \"\n",
    "                f\"It/sec {steps_per_report / (stop - start):.3f}, \"\n",
    "                f\"Tokens/sec {float(n_tokens) / (stop - start):.3f}\"\n",
    "            )\n",
    "            losses = []\n",
    "            n_tokens = 0\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Report validation loss if needed\n",
    "        if it == 0 or (it + 1) % steps_per_eval == 0:\n",
    "            stop = time.perf_counter()\n",
    "            val_loss = evaluate(\n",
    "                model, val_set, loss, tokenizer, batch_size, val_batches\n",
    "            )\n",
    "            print(\n",
    "                f\"Iter {it + 1}: \"\n",
    "                f\"Val loss {val_loss:.3f}, \"\n",
    "                f\"Val took {(time.perf_counter() - stop):.3f}s\"\n",
    "            )\n",
    "\n",
    "            start = time.perf_counter()\n",
    "\n",
    "        # Save adapter weights if needed\n",
    "        if (it + 1) % save_every == 0:\n",
    "            mx.savez(\n",
    "                adapter_file, **dict(tree_flatten(model.trainable_parameters()))\n",
    "            )\n",
    "            print(f\"Iter {it + 1}: Saved adapter weights to {adapter_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, inputs, targets, lengths):\n",
    "    # Run model on inputs\n",
    "    logits, _ = model(inputs)\n",
    "    logits = logits.astype(mx.float32)\n",
    "\n",
    "    # Mask padding tokens\n",
    "    length_mask = mx.arange(inputs.shape[1])[None, :] < lengths[:, None]\n",
    "\n",
    "    # Calculate the loss\n",
    "    ce = nn.losses.cross_entropy(logits, targets) * length_mask\n",
    "    ntoks = length_mask.sum()\n",
    "    ce = ce.sum() / ntoks\n",
    "    return ce, ntoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m opt \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Save adapter weights\u001b[39;00m\n\u001b[1;32m     11\u001b[0m mx\u001b[38;5;241m.\u001b[39msavez(adapter_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(tree_flatten(model\u001b[38;5;241m.\u001b[39mtrainable_parameters())))\n",
      "Cell \u001b[0;32mIn[89], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_set, val_set, optimizer, loss, tokenizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mrange\u001b[39m(iters),\n\u001b[1;32m     12\u001b[0m     iterate_batches(train_set, tokenizer, batch_size, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     13\u001b[0m ):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Forward and backward pass\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     (lvalue, toks), grad \u001b[38;5;241m=\u001b[39m \u001b[43mloss_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Model update\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mupdate(model, grad)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/mlx/nn/utils.py:34\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.wrapped_value_grad_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_value_grad_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 34\u001b[0m     value, grad \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value, grad\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/mlx/nn/utils.py:28\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.inner_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_fn\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m.\u001b[39mupdate(params)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[90], line 3\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(model, inputs, targets, lengths)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(model, inputs, targets, lengths):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Run model on inputs\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mastype(mx\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Mask padding tokens\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "opt = optim.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Train model\n",
    "train(model, train_set, valid_set, opt, loss, tokenizer)\n",
    "\n",
    "# Save adapter weights\n",
    "mx.savez(adapter_file, **dict(tree_flatten(model.trainable_parameters())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
